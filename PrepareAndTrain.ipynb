{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "We are going to try to predict the likelihood that an HIV patient\u0027s infection will become less\nsevere, given a small dataset and limited clinical information.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "This is a simple classification problem using csv data. Lets start by taking a look at the data\nand preparing it if necessary.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Model, Input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, Embedding, LSTM, concatenate\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras_tqdm import TQDMCallback\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "   PatientID  Resp                                             PR Seq  \\\n0          1     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...   \n1          2     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...   \n2          3     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...   \n3          4     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...   \n4          5     0  CCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...   \n\n                                              RT Seq  VL-t0  CD4-t0  \n0  CCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...    4.3     145  \n1  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.6     224  \n2  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.2    1017  \n3  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    5.7     206  \n4  CCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    3.5     572  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePatientID\u003c/th\u003e\n      \u003cth\u003eResp\u003c/th\u003e\n      \u003cth\u003ePR Seq\u003c/th\u003e\n      \u003cth\u003eRT Seq\u003c/th\u003e\n      \u003cth\u003eVL-t0\u003c/th\u003e\n      \u003cth\u003eCD4-t0\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eCCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAG...\u003c/td\u003e\n      \u003ctd\u003eCCCATTAGTCCTATTGAAACTGTACCAGTAAAGCTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e4.3\u003c/td\u003e\n      \u003ctd\u003e145\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eCCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGATAG...\u003c/td\u003e\n      \u003ctd\u003eCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e3.6\u003c/td\u003e\n      \u003ctd\u003e224\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eCCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAAGGTAG...\u003c/td\u003e\n      \u003ctd\u003eCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e3.2\u003c/td\u003e\n      \u003ctd\u003e1017\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eCCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAATAAGGATAG...\u003c/td\u003e\n      \u003ctd\u003eCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e5.7\u003c/td\u003e\n      \u003ctd\u003e206\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eCCTCAAATCACTCTTTGGCAACGACCCCTCGTCGCAGTAAAGATAG...\u003c/td\u003e\n      \u003ctd\u003eCCCATTAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e3.5\u003c/td\u003e\n      \u003ctd\u003e572\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 2
        }
      ],
      "source": "X \u003d pd.read_csv(\u0027training_data.csv\u0027)\nX.head()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Col-1: patient ID  \n\nCol-2: responder status (\"1\" for patients who improved and \"0\" otherwise)\n\nCol-3: Protease nucleotide sequence (if available)\n\nCol-4: Reverse Transciptase nucleotide sequence (if available)\n\nCol-5: viral load at the beginning of therapy (log-10 units)\n\nCol-6: CD4 count at the beginning of therapy\n\nThe Responder status indicates whether the patient improved after 16 weeks of therapy. \nImprovement is defined as a 100-fold decrease in the HIV-1 viral load.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We only have three numerical variables, but it still might be interesting to look at their correlations\n  ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "\u003cmatplotlib.axes._subplots.AxesSubplot at 0x1ca17615710\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 3
        },
        {
          "data": {
            "text/plain": "\u003cFigure size 792x648 with 2 Axes\u003e",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": "correlation_matrix \u003d X.loc[:, X.columns !\u003d \u0027PatientID\u0027].corr()\nmask \u003d np.zeros_like(correlation_matrix, dtype\u003dnp.bool)\nmask[np.triu_indices_from(mask)] \u003d True\n\nf, ax \u003d plt.subplots(figsize\u003d(11, 9))\n\ncmap \u003d sns.diverging_palette(220, 10, as_cmap\u003dTrue)\n\nsns.heatmap(correlation_matrix, mask\u003dmask, cmap\u003dcmap, vmax\u003d0.5, center\u003d0, linewidth\u003d.5, cbar\u003d{\u0027shrink\u0027:.5})\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Response correlation with CD4 \u003d -0.11862579883264826",
            "\n",
            "Response correlation with viral load at start \u003d 0.3623997929382554",
            "\n",
            "Viral load correlation with CD4 \u003d -0.4161793739610801",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(f\"Response correlation with CD4 \u003d {X[\u0027Resp\u0027].corr(X[\u0027CD4-t0\u0027])}\")\nprint(f\"Response correlation with viral load at start \u003d {X[\u0027Resp\u0027].corr(X[\u0027VL-t0\u0027])}\")\nprint(f\"Viral load correlation with CD4 \u003d {X[\u0027VL-t0\u0027].corr(X[\u0027CD4-t0\u0027])}\")\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "It seems the more of CD4 glycopretein is found at the start of the treatment the lesser the viral load is by a\nfactor of 0.42, which is pretty high.\n\nAnd interestingly enough, the greater the viral load at the start, the greater the response to\ntreatment,  (which might imply that it is easier to drastically reduce viral load rather than\nto completely remove it once it is already low).\n ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We can now try to take a look at the variables containing the nucleotide sequences.\nSince they are not numerical, and I cannot interpret them, I\u0027ll just take a look to see how unique they are.\n(They should be unique, but lets just double check)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Total number of sequences \u003d 1000",
            "\n",
            "Number of unique Protease sequences \u003d 921",
            "\n",
            "Number of unique Reverse Transciptase sequences \u003d 1000",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(f\"Total number of sequences \u003d {len(X[\u0027PR Seq\u0027])}\")\n\nprint(f\"Number of unique Protease sequences \u003d {len(X[\u0027PR Seq\u0027].unique())}\")\n\nprint(f\"Number of unique Reverse Transciptase sequences \u003d {len(X[\u0027RT Seq\u0027].unique())}\")\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since they are mostly unique we will have to try to find smaller patterns inside of them, otherwise the model will\nbasically ignore them, as comparing the full strings to each other will result in almost no correlation.\n\nWe can either try to divide them into codons (Three-letter \u0027words\u0027) formed from a sequence\nof three nucleotides (e.g. \u0027ACT\u0027, \u0027CAG\u0027, \u0027TTT\u0027) or just feed the string as a sequence of\ncharacters, leaving the data a little more unstructured, which might be even better, since the\nless domain knowledge we pass to neural networks, the better they usually work,\nas they are made to find patterns, and they are way better at it than we are.\n\nAlso since this type of data should be unique we are probably dealing with missing values.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "     PatientID  Resp PR Seq  \\\n920        921     0    NaN   \n921        922     0    NaN   \n922        923     0    NaN   \n923        924     1    NaN   \n924        925     0    NaN   \n\n                                                RT Seq  VL-t0  CD4-t0  \n920  GAAATSTGTACAGAAATGSAAAAGGAAGGGAAAATTTCAAAAATTG...    4.1     227  \n921  AAATTAAAGCCAGGAATGGATGGCCCAAAGGTTAAACAATGGCCAT...    3.2     287  \n922  AGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATG...    3.0     638  \n923  CCCATAAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    4.5     699  \n924  CCCATAAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...    2.7     299  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePatientID\u003c/th\u003e\n      \u003cth\u003eResp\u003c/th\u003e\n      \u003cth\u003ePR Seq\u003c/th\u003e\n      \u003cth\u003eRT Seq\u003c/th\u003e\n      \u003cth\u003eVL-t0\u003c/th\u003e\n      \u003cth\u003eCD4-t0\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e920\u003c/th\u003e\n      \u003ctd\u003e921\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003eGAAATSTGTACAGAAATGSAAAAGGAAGGGAAAATTTCAAAAATTG...\u003c/td\u003e\n      \u003ctd\u003e4.1\u003c/td\u003e\n      \u003ctd\u003e227\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e921\u003c/th\u003e\n      \u003ctd\u003e922\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003eAAATTAAAGCCAGGAATGGATGGCCCAAAGGTTAAACAATGGCCAT...\u003c/td\u003e\n      \u003ctd\u003e3.2\u003c/td\u003e\n      \u003ctd\u003e287\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e922\u003c/th\u003e\n      \u003ctd\u003e923\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003eAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAATGGATG...\u003c/td\u003e\n      \u003ctd\u003e3.0\u003c/td\u003e\n      \u003ctd\u003e638\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e923\u003c/th\u003e\n      \u003ctd\u003e924\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003eCCCATAAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e4.5\u003c/td\u003e\n      \u003ctd\u003e699\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e924\u003c/th\u003e\n      \u003ctd\u003e925\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n      \u003ctd\u003eCCCATAAGTCCTATTGAAACTGTACCAGTAAAATTAAAGCCAGGAA...\u003c/td\u003e\n      \u003ctd\u003e2.7\u003c/td\u003e\n      \u003ctd\u003e299\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 6
        }
      ],
      "source": "duplicated_items \u003d X[X[\u0027PR Seq\u0027].duplicated(keep\u003dFalse)]\nduplicated_items.head()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "As expected, we have some missing values.\nLets see how many we have in order to decide what to do with them.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "80"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 7
        }
      ],
      "source": "len(duplicated_items)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since only 80 out of 1000 variables are missing we are just going to drop these rows, its not worth it trying to \nsalvage the rest of the information on these patients as there are so few of them.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "920"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 8
        }
      ],
      "source": "X \u003d X.dropna()\nlen(X)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We should also check if the dataset is unbalanced (more instances of a class over another).\nIf so, the model will become stuck at a local optima predicting only the most prevalent class.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "0.20326086956521738"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 9
        }
      ],
      "source": "y \u003d np.asarray(X[[\u0027Resp\u0027]])\npercent_positives \u003d np.count_nonzero(y) / len(y)\npercent_positives\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Only around 20% of the dataset is positive, one of the simplest ways to solve this is to \nresample the dataset by creating 4 copies of each positive entry.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "1481"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 10
        }
      ],
      "source": "reps \u003d [4 if val \u003d\u003d 1 else 1 for val in X[\u0027Resp\u0027]]\nX \u003d X.loc[np.repeat(X.index.values, reps)]\nlen(X)\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since this data is sequential we are going to need to keep the current structure of the data and pass it through \na model that can interpret the nuances of the sequences in their order, such as a RNN (recurrent neural network).\n\nIn this case we will use a LSTM (long short term memory) network, which is a type of RNN.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Lets start by converting the characters into integers. Since we know there are only a few possible characters\nwe can create our own conversion algorithm rather than using their respective ASCII codes as it is usually done\nor this type of problem, to save time and memory space.\n\nWhile A, T, C, and G represent a particular nucleotide at a position, there are also letters that represent\nambiguity which are used when more than one kind of nucleotide could occur at that position.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "   PatientID  Resp                                             PR Seq  \\\n0          1     0  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n1          2     0  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n2          3     0  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n3          4     0  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n4          5     0  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n\n                                              RT Seq  VL-t0  CD4-t0  \n0  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    4.3     145  \n1  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    3.6     224  \n2  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    3.2    1017  \n3  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    5.7     206  \n4  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    3.5     572  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePatientID\u003c/th\u003e\n      \u003cth\u003eResp\u003c/th\u003e\n      \u003cth\u003ePR Seq\u003c/th\u003e\n      \u003cth\u003eRT Seq\u003c/th\u003e\n      \u003cth\u003eVL-t0\u003c/th\u003e\n      \u003cth\u003eCD4-t0\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.3\u003c/td\u003e\n      \u003ctd\u003e145\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.6\u003c/td\u003e\n      \u003ctd\u003e224\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.2\u003c/td\u003e\n      \u003ctd\u003e1017\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e5.7\u003c/td\u003e\n      \u003ctd\u003e206\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.5\u003c/td\u003e\n      \u003ctd\u003e572\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 11
        }
      ],
      "source": "def clean(x):\n    x \u003d x.replace(\"A\", \"1\").replace(\"T\", \"2\").replace(\"C\", \"3\").replace(\"G\", \"4\").replace(\"U\", \"5\").replace(\"W\", \"6\").replace(\"S\", \"7\").replace(\"M\", \"8\").replace(\"K\", \"9\").replace(\"R\", \"10\").replace(\"Y\", \"11\").replace(\"B\", \"12\").replace(\"D\", \"13\").replace(\"H\", \"14\").replace(\"V\", \"15\").replace(\"N\", \"16\").replace(\"Z\", \"17\")\n    return x\n    \ndef transform_to_array(x):\n    x \u003d np.asarray(list(x), dtype\u003dnp.uint8)\n    return x\n\nX[\u0027PR Seq\u0027] \u003d X[\u0027PR Seq\u0027].apply(clean)\nX[\u0027PR Seq\u0027] \u003d X[\u0027PR Seq\u0027].apply(transform_to_array)\n\nX[\u0027RT Seq\u0027] \u003d X[\u0027RT Seq\u0027].apply(clean)\nX[\u0027RT Seq\u0027] \u003d X[\u0027RT Seq\u0027].apply(transform_to_array)\n    \nX.head()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Lets also check if the length of the sequences is variable in order to pass them properly to the model later.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "PR: \n",
            "\n",
            "0    301\n1    297\n2    301\n3    298\n4    297\n5    297\n6    299\n7    297\n8    297\n9    298\nName: PR Seq, dtype: int64",
            "\n",
            "\nRT: \n",
            "\n",
            "0    1010\n1     913\n2     917\n3    1457\n4     910\n5     902\n6     915\n7     914\n8     909\n9     901\nName: RT Seq, dtype: int64",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(\u0027PR: \\n\u0027)\nprint(X[\u0027PR Seq\u0027][:10].apply(len))\nprint(\"\\nRT: \\n\")\nprint(X[\u0027RT Seq\u0027][:10].apply(len))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The lengths are variable. We need to pad them with zeroes to create sequences of invariable length in order to\nfeed them as inputs to our network. Luckily Keras has a function just for that.\n\nlets check their max lengths in order to define the size of the padded sequences.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "PR max \u003d 312",
            "\n",
            "RT max \u003d 1575",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "max_PR \u003d np.amax(X[\u0027PR Seq\u0027].apply(len))\nprint(f\u0027PR max \u003d {max_PR}\u0027)\n\nmax_RT \u003d np.amax(X[\u0027RT Seq\u0027].apply(len))\nprint(f\u0027RT max \u003d {max_RT}\u0027)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Lets split the dataset into train and test partitions.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": "train, test \u003d train_test_split(X, test_size\u003d0.25)\n\ny_train \u003d train[[\u0027Resp\u0027]]\nX_train \u003d train.drop(\u0027Resp\u0027, axis\u003d1)\n\ny_test \u003d test[[\u0027Resp\u0027]]\nX_test \u003d test.drop(\u0027Resp\u0027, axis\u003d1)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/plain": "     PatientID                                             PR Seq  \\\n280        281  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n591        592  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n585        586  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n416        417  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n841        842  [3, 3, 2, 3, 1, 1, 0, 1, 2, 3, 1, 3, 2, 3, 2, ...   \n\n                                                RT Seq  VL-t0  CD4-t0  \n280  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    3.7      77  \n591  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    5.2     170  \n585  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    5.0      75  \n416  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    4.8      62  \n841  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...    3.6     533  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePatientID\u003c/th\u003e\n      \u003cth\u003ePR Seq\u003c/th\u003e\n      \u003cth\u003eRT Seq\u003c/th\u003e\n      \u003cth\u003eVL-t0\u003c/th\u003e\n      \u003cth\u003eCD4-t0\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e280\u003c/th\u003e\n      \u003ctd\u003e281\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.7\u003c/td\u003e\n      \u003ctd\u003e77\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e591\u003c/th\u003e\n      \u003ctd\u003e592\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e5.2\u003c/td\u003e\n      \u003ctd\u003e170\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e585\u003c/th\u003e\n      \u003ctd\u003e586\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e5.0\u003c/td\u003e\n      \u003ctd\u003e75\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e416\u003c/th\u003e\n      \u003ctd\u003e417\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.8\u003c/td\u003e\n      \u003ctd\u003e62\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e841\u003c/th\u003e\n      \u003ctd\u003e842\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 0, 1, 2, 3, 1, 3, 2, 3, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.6\u003c/td\u003e\n      \u003ctd\u003e533\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 15
        }
      ],
      "source": "X_train.head()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "data": {
            "text/plain": "     Resp\n280     0\n591     1\n585     1\n416     0\n841     0",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eResp\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e280\u003c/th\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e591\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e585\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e416\u003c/th\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e841\u003c/th\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 16
        }
      ],
      "source": "y_train.head()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/plain": "     PatientID                                             PR Seq  \\\n642        643  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n735        736  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n389        390  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n125        126  [3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n642        643  [3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...   \n\n                                                RT Seq  VL-t0  CD4-t0  \n642  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...   4.55     104  \n735  [3, 3, 3, 1, 2, 2, 1, 4, 3, 3, 3, 2, 1, 2, 2, ...   4.90      13  \n389  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...   3.55     238  \n125  [3, 3, 1, 1, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, ...   4.15     306  \n642  [3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...   4.55     104  ",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePatientID\u003c/th\u003e\n      \u003cth\u003ePR Seq\u003c/th\u003e\n      \u003cth\u003eRT Seq\u003c/th\u003e\n      \u003cth\u003eVL-t0\u003c/th\u003e\n      \u003cth\u003eCD4-t0\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e642\u003c/th\u003e\n      \u003ctd\u003e643\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.55\u003c/td\u003e\n      \u003ctd\u003e104\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e735\u003c/th\u003e\n      \u003ctd\u003e736\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 3, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.90\u003c/td\u003e\n      \u003ctd\u003e13\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e389\u003c/th\u003e\n      \u003ctd\u003e390\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e3.55\u003c/td\u003e\n      \u003ctd\u003e238\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e125\u003c/th\u003e\n      \u003ctd\u003e126\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 1, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 1, 1, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.15\u003c/td\u003e\n      \u003ctd\u003e306\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e642\u003c/th\u003e\n      \u003ctd\u003e643\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 2, 3, 1, 4, 1, 2, 3, 1, 3, 2, 3, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e[3, 3, 3, 1, 2, 2, 1, 4, 2, 3, 3, 2, 1, 2, 2, ...\u003c/td\u003e\n      \u003ctd\u003e4.55\u003c/td\u003e\n      \u003ctd\u003e104\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 17
        }
      ],
      "source": "X_test.head()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "     Resp\n642     1\n735     1\n389     0\n125     0\n642     1",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eResp\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e642\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e735\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e389\u003c/th\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e125\u003c/th\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e642\u003c/th\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 18
        }
      ],
      "source": "y_test.head()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "And now to pad them with zeroes to get constant lengths for the model\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": "padded_PR_train \u003d sequence.pad_sequences(X_train[\u0027PR Seq\u0027], maxlen\u003dmax_PR, dtype\u003dnp.uint8)\n\npadded_RT_train \u003d sequence.pad_sequences(X_train[\u0027RT Seq\u0027], maxlen\u003dmax_RT, dtype\u003dnp.uint8)\n\npadded_PR_test \u003d sequence.pad_sequences(X_test[\u0027PR Seq\u0027], maxlen\u003dmax_PR, dtype\u003dnp.uint8)\n\npadded_RT_test \u003d sequence.pad_sequences(X_test[\u0027RT Seq\u0027], maxlen\u003dmax_RT, dtype\u003dnp.uint8)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "pr_train \u003d\n[[0 0 0 ... 2 2 2]\n [0 0 0 ... 2 2 2]\n [0 0 0 ... 2 2 2]\n [0 0 0 ... 2 2 2]\n [0 0 0 ... 2 2 2]],\nrt_train \u003d\n[[0 0 0 ... 4 4 4]\n [0 0 0 ... 4 1 2]\n [0 0 0 ... 4 1 1]\n [0 0 0 ... 4 1 2]\n [0 0 0 ... 4 4 3]],\npr_test \u003d\n[[0 0 0 ... 2 2 3]\n [0 0 0 ... 2 2 2]\n [0 0 0 ... 2 2 2]\n [0 0 0 ... 2 1 1]\n [0 0 0 ... 2 2 3]],\n rt_test \u003d\n[[0 0 0 ... 3 1 4]\n [0 0 0 ... 4 4 3]\n [0 0 0 ... 3 1 4]\n [0 0 0 ... 3 2 4]\n [0 0 0 ... 3 1 4]]",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(f\u0027pr_train \u003d\\n{padded_PR_train[0:5]},\\nrt_train \u003d\\n{padded_RT_train[0:5]},\\n\u0027\n      f\u0027pr_test \u003d\\n{padded_PR_test[0:5]},\\n rt_test \u003d\\n{padded_RT_test[0:5]}\u0027)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Everything looks good so far, now we only need to build our binary classifier model.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We are going to build a wide model with two separate LSTM nodes and a simple fully connected node that merge at\nend before passing a final fully connected layer and the actual prediction layer.\n\n        ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "early_stopping \u003d EarlyStopping(monitor\u003d\u0027val_loss\u0027, patience\u003d5)\ncheckpoint \u003d ModelCheckpoint(filepath\u003d\u0027weights.hdf5\u0027, verbose\u003d1, save_best_only\u003dTrue)\n\nepochs \u003d 1000\nbatch_size \u003d 16\n\nvocab_size \u003d 18 # There are 17 possible letters used on the sequences\nembedding_vector_length \u003d 128 # One of the hyperparameters to possibly be tuned, I chose this one almost arbitrarily,\n# Just picking a small multiple of two for performance \n\ninput_a \u003d Input(shape\u003d(max_PR, ))\ninput_b \u003d Input(shape\u003d(max_RT, ))\ninput_c \u003d Input(shape\u003d(2, ))\n\nx1 \u003d Embedding(vocab_size, embedding_vector_length)(input_a)\nx1 \u003d LSTM(100)(x1)\nx1 \u003d Model(inputs\u003dinput_a, outputs\u003dx1)\n\nx2 \u003d Embedding(vocab_size, embedding_vector_length)(input_b)\nx2 \u003d LSTM(100)(x2)\nx2 \u003d Model(inputs\u003dinput_b, outputs\u003dx2)\n\nx3 \u003d Dense(100, activation\u003d\u0027relu\u0027)(input_c)\nx3 \u003d Model(inputs\u003dinput_c, outputs\u003dx3)\n\ncombined \u003d concatenate([x1.output, x2.output, x3.output])\n\nfully_connected  \u003d Dense(128, activation\u003d\u0027relu\u0027)(combined)\nprediction \u003d Dense(1, activation\u003d\u0027sigmoid\u0027)(fully_connected)\n\nmodel \u003d Model(inputs\u003d[x1.input, x2.input, x3.input], outputs\u003dprediction)\n\nwith open(\u0027model2_architecture.json\u0027, \u0027w\u0027) as f:\n    f.write(model.to_json())\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "__________________________________________________________________________________________________",
            "\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     ",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
            "\n",
            "input_1 (InputLayer)            (None, 312)          0                                            ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "input_2 (InputLayer)            (None, 1575)         0                                            ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "embedding_1 (Embedding)         (None, 312, 128)     2304        input_1[0][0]                    ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "embedding_2 (Embedding)         (None, 1575, 128)    2304        input_2[0][0]                    ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "input_3 (InputLayer)            (None, 2)            0                                            ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "lstm_1 (LSTM)                   (None, 100)          91600       embedding_1[0][0]                ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "lstm_2 (LSTM)                   (None, 100)          91600       embedding_2[0][0]                ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "dense_1 (Dense)                 (None, 100)          300         input_3[0][0]                    ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "concatenate_1 (Concatenate)     (None, 300)          0           lstm_1[0][0]                     ",
            "\n",
            "                                                                 lstm_2[0][0]                     ",
            "\n",
            "                                                                 dense_1[0][0]                    ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "dense_2 (Dense)                 (None, 128)          38528       concatenate_1[0][0]              ",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "dense_3 (Dense)                 (None, 1)            129         dense_2[0][0]                    ",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
            "\n",
            "Total params: 226,765",
            "\n",
            "Trainable params: 226,765",
            "\n",
            "Non-trainable params: 0",
            "\n",
            "__________________________________________________________________________________________________",
            "\n",
            "None",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "model.compile(loss\u003d\u0027binary_crossentropy\u0027, optimizer\u003dAdam(lr\u003d0.0003), metrics\u003d[\u0027accuracy\u0027])\nprint(model.summary())\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "\nEpoch 00001: val_loss improved from inf to 0.73739, saving model to weights.hdf5",
            "\n",
            "\nEpoch 00002: val_loss improved from 0.73739 to 0.65623, saving model to weights.hdf5",
            "\n",
            "\nEpoch 00003: val_loss improved from 0.65623 to 0.64252, saving model to weights.hdf5",
            "\n",
            "\nEpoch 00004: val_loss did not improve from 0.64252",
            "\n",
            "\nEpoch 00005: val_loss improved from 0.64252 to 0.61527, saving model to weights.hdf5",
            "\n",
            "\nEpoch 00006: val_loss did not improve from 0.61527",
            "\n",
            "\nEpoch 00007: val_loss did not improve from 0.61527",
            "\n",
            "\nEpoch 00008: val_loss did not improve from 0.61527",
            "\n",
            "\nEpoch 00009: val_loss did not improve from 0.61527",
            "\n",
            "\nEpoch 00010: val_loss did not improve from 0.61527",
            "\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1000 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 0:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 0 - loss: 4.470, acc: 0.625:   1%|▏         | 16/1110 [00:12\u003c14:00,  1.30it/s]",
            "\n",
            "\rEpoch: 0 - loss: 4.510, acc: 0.594:   3%|▎         | 32/1110 [00:21\u003c12:41,  1.42it/s]",
            "\n",
            "\rEpoch: 0 - loss: 4.751, acc: 0.521:   4%|▍         | 48/1110 [00:30\u003c11:44,  1.51it/s]",
            "\n",
            "\rEpoch: 0 - loss: 4.202, acc: 0.531:   6%|▌         | 64/1110 [00:38\u003c10:51,  1.60it/s]",
            "\n",
            "\rEpoch: 0 - loss: 3.534, acc: 0.512:   7%|▋         | 80/1110 [00:46\u003c09:53,  1.74it/s]",
            "\n",
            "\rEpoch: 0 - loss: 3.291, acc: 0.490:   9%|▊         | 96/1110 [00:55\u003c09:37,  1.76it/s]",
            "\n",
            "\rEpoch: 0 - loss: 3.152, acc: 0.491:  10%|█         | 112/1110 [01:03\u003c09:19,  1.78it/s]",
            "\n",
            "\rEpoch: 0 - loss: 3.158, acc: 0.484:  12%|█▏        | 128/1110 [01:12\u003c09:01,  1.81it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.939, acc: 0.479:  13%|█▎        | 144/1110 [01:20\u003c08:49,  1.83it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.749, acc: 0.469:  14%|█▍        | 160/1110 [01:29\u003c08:44,  1.81it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.593, acc: 0.460:  16%|█▌        | 176/1110 [01:39\u003c08:44,  1.78it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.459, acc: 0.479:  17%|█▋        | 192/1110 [01:47\u003c08:29,  1.80it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.351, acc: 0.476:  19%|█▊        | 208/1110 [01:57\u003c08:28,  1.78it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.259, acc: 0.469:  20%|██        | 224/1110 [02:04\u003c07:58,  1.85it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.201, acc: 0.471:  22%|██▏       | 240/1110 [02:13\u003c07:45,  1.87it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.103, acc: 0.488:  23%|██▎       | 256/1110 [02:25\u003c08:32,  1.67it/s]",
            "\n",
            "\rEpoch: 0 - loss: 2.017, acc: 0.493:  25%|██▍       | 272/1110 [02:34\u003c08:12,  1.70it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.944, acc: 0.497:  26%|██▌       | 288/1110 [02:45\u003c08:29,  1.61it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.907, acc: 0.480:  27%|██▋       | 304/1110 [02:56\u003c08:32,  1.57it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.852, acc: 0.481:  29%|██▉       | 320/1110 [03:06\u003c08:31,  1.55it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.798, acc: 0.479:  30%|███       | 336/1110 [03:16\u003c08:03,  1.60it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.743, acc: 0.489:  32%|███▏      | 352/1110 [03:24\u003c07:37,  1.66it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.721, acc: 0.478:  33%|███▎      | 368/1110 [03:32\u003c07:03,  1.75it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.690, acc: 0.477:  35%|███▍      | 384/1110 [03:41\u003c06:43,  1.80it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.656, acc: 0.477:  36%|███▌      | 400/1110 [03:48\u003c06:17,  1.88it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.636, acc: 0.474:  37%|███▋      | 416/1110 [03:56\u003c05:59,  1.93it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.602, acc: 0.470:  39%|███▉      | 432/1110 [04:03\u003c05:35,  2.02it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.573, acc: 0.473:  40%|████      | 448/1110 [04:12\u003c05:36,  1.97it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.551, acc: 0.472:  42%|████▏     | 464/1110 [04:20\u003c05:31,  1.95it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.522, acc: 0.477:  43%|████▎     | 480/1110 [04:29\u003c05:32,  1.90it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.500, acc: 0.480:  45%|████▍     | 496/1110 [04:37\u003c05:22,  1.90it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.475, acc: 0.482:  46%|████▌     | 512/1110 [04:46\u003c05:17,  1.88it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.451, acc: 0.485:  48%|████▊     | 528/1110 [04:54\u003c05:02,  1.93it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.431, acc: 0.487:  49%|████▉     | 544/1110 [05:01\u003c04:42,  2.00it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.415, acc: 0.484:  50%|█████     | 560/1110 [05:10\u003c04:45,  1.93it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.397, acc: 0.483:  52%|█████▏    | 576/1110 [05:18\u003c04:35,  1.94it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.377, acc: 0.490:  53%|█████▎    | 592/1110 [05:27\u003c04:28,  1.93it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.359, acc: 0.488:  55%|█████▍    | 608/1110 [05:35\u003c04:16,  1.96it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.343, acc: 0.487:  56%|█████▌    | 624/1110 [05:43\u003c04:12,  1.92it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.329, acc: 0.486:  58%|█████▊    | 640/1110 [05:51\u003c04:01,  1.95it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.313, acc: 0.485:  59%|█████▉    | 656/1110 [05:59\u003c03:48,  1.99it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.296, acc: 0.491:  61%|██████    | 672/1110 [06:08\u003c03:49,  1.91it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.278, acc: 0.500:  62%|██████▏   | 688/1110 [06:17\u003c03:41,  1.90it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.268, acc: 0.501:  63%|██████▎   | 704/1110 [06:25\u003c03:34,  1.89it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.254, acc: 0.501:  65%|██████▍   | 720/1110 [06:34\u003c03:26,  1.89it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.243, acc: 0.504:  66%|██████▋   | 736/1110 [06:41\u003c03:07,  2.00it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.235, acc: 0.507:  68%|██████▊   | 752/1110 [06:49\u003c03:00,  1.99it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.222, acc: 0.508:  69%|██████▉   | 768/1110 [06:57\u003c02:52,  1.99it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.210, acc: 0.509:  71%|███████   | 784/1110 [07:05\u003c02:43,  1.99it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.202, acc: 0.507:  72%|███████▏  | 800/1110 [07:13\u003c02:37,  1.97it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.194, acc: 0.505:  74%|███████▎  | 816/1110 [07:22\u003c02:31,  1.94it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.183, acc: 0.505:  75%|███████▍  | 832/1110 [07:30\u003c02:24,  1.92it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.173, acc: 0.508:  76%|███████▋  | 848/1110 [07:38\u003c02:13,  1.96it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.166, acc: 0.506:  78%|███████▊  | 864/1110 [07:46\u003c02:06,  1.94it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.157, acc: 0.507:  79%|███████▉  | 880/1110 [07:55\u003c01:58,  1.94it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.148, acc: 0.510:  81%|████████  | 896/1110 [08:01\u003c01:42,  2.09it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.142, acc: 0.511:  82%|████████▏ | 912/1110 [08:10\u003c01:40,  1.97it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.132, acc: 0.515:  84%|████████▎ | 928/1110 [08:19\u003c01:35,  1.90it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.125, acc: 0.513:  85%|████████▌ | 944/1110 [08:28\u003c01:27,  1.89it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.118, acc: 0.514:  86%|████████▋ | 960/1110 [08:36\u003c01:18,  1.91it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.111, acc: 0.513:  88%|████████▊ | 976/1110 [08:45\u003c01:10,  1.89it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.104, acc: 0.514:  89%|████████▉ | 992/1110 [08:53\u003c01:02,  1.88it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.096, acc: 0.515:  91%|█████████ | 1008/1110 [09:02\u003c00:53,  1.89it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.090, acc: 0.514:  92%|█████████▏| 1024/1110 [09:11\u003c00:46,  1.83it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.083, acc: 0.514:  94%|█████████▎| 1040/1110 [09:20\u003c00:37,  1.85it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.076, acc: 0.516:  95%|█████████▌| 1056/1110 [09:28\u003c00:29,  1.86it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.070, acc: 0.518:  97%|█████████▋| 1072/1110 [09:37\u003c00:20,  1.85it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.072, acc: 0.517:  98%|█████████▊| 1088/1110 [09:45\u003c00:11,  1.88it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.067, acc: 0.516:  99%|█████████▉| 1104/1110 [09:52\u003c00:03,  1.95it/s]",
            "\n",
            "\rEpoch: 0 - loss: 1.067, acc: 0.515, val_loss: 0.737, val_acc: 0.536: 100%|██████████| 1110/1110 [11:03\u003c00:00,  3.87s/it]",
            "\rTraining:   0%|          | 1/1000 [11:03\u003c184:01:13, 663.14s/it]",
            "\n",
            "\rEpoch: 1:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.805, acc: 0.438:   1%|▏         | 16/1110 [00:08\u003c09:52,  1.85it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.719, acc: 0.594:   3%|▎         | 32/1110 [00:17\u003c09:37,  1.87it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.735, acc: 0.562:   4%|▍         | 48/1110 [00:25\u003c09:24,  1.88it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.714, acc: 0.594:   6%|▌         | 64/1110 [00:34\u003c09:20,  1.86it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.742, acc: 0.562:   7%|▋         | 80/1110 [00:42\u003c08:58,  1.91it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.754, acc: 0.531:   9%|▊         | 96/1110 [00:50\u003c08:43,  1.94it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.743, acc: 0.527:  10%|█         | 112/1110 [00:58\u003c08:32,  1.95it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.731, acc: 0.523:  12%|█▏        | 128/1110 [01:06\u003c08:24,  1.94it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.726, acc: 0.521:  13%|█▎        | 144/1110 [01:15\u003c08:25,  1.91it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.719, acc: 0.525:  14%|█▍        | 160/1110 [01:22\u003c08:03,  1.97it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.714, acc: 0.528:  16%|█▌        | 176/1110 [01:30\u003c07:54,  1.97it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.711, acc: 0.536:  17%|█▋        | 192/1110 [01:38\u003c07:45,  1.97it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.744, acc: 0.514:  19%|█▊        | 208/1110 [01:47\u003c07:39,  1.96it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.740, acc: 0.522:  20%|██        | 224/1110 [01:54\u003c07:18,  2.02it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.740, acc: 0.517:  22%|██▏       | 240/1110 [02:01\u003c06:58,  2.08it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.742, acc: 0.512:  23%|██▎       | 256/1110 [02:08\u003c06:43,  2.12it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.742, acc: 0.511:  25%|██▍       | 272/1110 [02:15\u003c06:21,  2.19it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.743, acc: 0.500:  26%|██▌       | 288/1110 [02:23\u003c06:22,  2.15it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.738, acc: 0.503:  27%|██▋       | 304/1110 [02:32\u003c06:40,  2.01it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.726, acc: 0.516:  29%|██▉       | 320/1110 [02:41\u003c06:42,  1.96it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.717, acc: 0.530:  30%|███       | 336/1110 [02:49\u003c06:41,  1.93it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.733, acc: 0.534:  32%|███▏      | 352/1110 [02:58\u003c06:40,  1.89it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.763, acc: 0.535:  33%|███▎      | 368/1110 [03:07\u003c06:34,  1.88it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.799, acc: 0.529:  35%|███▍      | 384/1110 [03:15\u003c06:21,  1.90it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.793, acc: 0.535:  36%|███▌      | 400/1110 [03:23\u003c06:15,  1.89it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.789, acc: 0.538:  37%|███▋      | 416/1110 [03:31\u003c05:49,  1.98it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.791, acc: 0.539:  39%|███▉      | 432/1110 [03:37\u003c05:24,  2.09it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.790, acc: 0.545:  40%|████      | 448/1110 [03:45\u003c05:15,  2.10it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.796, acc: 0.547:  42%|████▏     | 464/1110 [03:53\u003c05:16,  2.04it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.809, acc: 0.544:  43%|████▎     | 480/1110 [04:04\u003c05:42,  1.84it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.817, acc: 0.544:  45%|████▍     | 496/1110 [04:13\u003c05:38,  1.82it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.811, acc: 0.549:  46%|████▌     | 512/1110 [04:22\u003c05:32,  1.80it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.807, acc: 0.553:  48%|████▊     | 528/1110 [04:32\u003c05:31,  1.75it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.806, acc: 0.557:  49%|████▉     | 544/1110 [04:42\u003c05:33,  1.70it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.827, acc: 0.555:  50%|█████     | 560/1110 [04:50\u003c05:09,  1.78it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.847, acc: 0.562:  52%|█████▏    | 576/1110 [04:58\u003c04:50,  1.84it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.851, acc: 0.562:  53%|█████▎    | 592/1110 [05:06\u003c04:35,  1.88it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.853, acc: 0.566:  55%|█████▍    | 608/1110 [05:14\u003c04:24,  1.90it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.866, acc: 0.564:  56%|█████▌    | 624/1110 [05:22\u003c04:09,  1.95it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.861, acc: 0.566:  58%|█████▊    | 640/1110 [05:30\u003c03:58,  1.97it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.860, acc: 0.569:  59%|█████▉    | 656/1110 [05:38\u003c03:48,  1.99it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.890, acc: 0.565:  61%|██████    | 672/1110 [05:45\u003c03:34,  2.05it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.902, acc: 0.565:  62%|██████▏   | 688/1110 [05:53\u003c03:30,  2.01it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.910, acc: 0.567:  63%|██████▎   | 704/1110 [06:01\u003c03:23,  2.00it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.912, acc: 0.567:  65%|██████▍   | 720/1110 [06:09\u003c03:13,  2.02it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.919, acc: 0.562:  66%|██████▋   | 736/1110 [06:17\u003c03:05,  2.02it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.912, acc: 0.564:  68%|██████▊   | 752/1110 [06:25\u003c02:58,  2.00it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.914, acc: 0.562:  69%|██████▉   | 768/1110 [06:32\u003c02:45,  2.06it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.932, acc: 0.560:  71%|███████   | 784/1110 [06:40\u003c02:38,  2.06it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.943, acc: 0.558:  72%|███████▏  | 800/1110 [06:48\u003c02:32,  2.03it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.951, acc: 0.555:  74%|███████▎  | 816/1110 [06:55\u003c02:20,  2.10it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.948, acc: 0.558:  75%|███████▍  | 832/1110 [07:03\u003c02:12,  2.10it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.950, acc: 0.557:  76%|███████▋  | 848/1110 [07:11\u003c02:05,  2.09it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.949, acc: 0.556:  78%|███████▊  | 864/1110 [07:18\u003c01:57,  2.10it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.942, acc: 0.558:  79%|███████▉  | 880/1110 [07:26\u003c01:48,  2.12it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.945, acc: 0.558:  81%|████████  | 896/1110 [07:34\u003c01:43,  2.06it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.952, acc: 0.557:  82%|████████▏ | 912/1110 [07:42\u003c01:35,  2.07it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.965, acc: 0.553:  84%|████████▎ | 928/1110 [07:49\u003c01:27,  2.09it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.964, acc: 0.552:  85%|████████▌ | 944/1110 [07:57\u003c01:19,  2.09it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.960, acc: 0.554:  86%|████████▋ | 960/1110 [08:05\u003c01:12,  2.07it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.957, acc: 0.552:  88%|████████▊ | 976/1110 [08:12\u003c01:05,  2.06it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.951, acc: 0.552:  89%|████████▉ | 992/1110 [08:21\u003c00:57,  2.04it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.952, acc: 0.551:  91%|█████████ | 1008/1110 [08:28\u003c00:50,  2.03it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.946, acc: 0.552:  92%|█████████▏| 1024/1110 [08:36\u003c00:42,  2.03it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.945, acc: 0.551:  94%|█████████▎| 1040/1110 [08:44\u003c00:34,  2.04it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.959, acc: 0.553:  95%|█████████▌| 1056/1110 [08:52\u003c00:26,  2.06it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.970, acc: 0.555:  97%|█████████▋| 1072/1110 [09:00\u003c00:18,  2.05it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.968, acc: 0.558:  98%|█████████▊| 1088/1110 [09:07\u003c00:10,  2.07it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.972, acc: 0.558:  99%|█████████▉| 1104/1110 [09:15\u003c00:02,  2.10it/s]",
            "\n",
            "\rEpoch: 1 - loss: 0.972, acc: 0.557, val_loss: 0.656, val_acc: 0.617: 100%|██████████| 1110/1110 [10:20\u003c00:00,  3.59s/it]",
            "\rTraining:   0%|          | 2/1000 [21:23\u003c180:15:58, 650.26s/it]",
            "\n",
            "\rEpoch: 2:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.657, acc: 0.500:   1%|▏         | 16/1110 [00:07\u003c08:58,  2.03it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.672, acc: 0.531:   3%|▎         | 32/1110 [00:15\u003c08:48,  2.04it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.870, acc: 0.458:   4%|▍         | 48/1110 [00:23\u003c08:49,  2.01it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.872, acc: 0.453:   6%|▌         | 64/1110 [00:31\u003c08:41,  2.01it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.835, acc: 0.463:   7%|▋         | 80/1110 [00:40\u003c08:42,  1.97it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.790, acc: 0.510:   9%|▊         | 96/1110 [00:47\u003c08:24,  2.01it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.825, acc: 0.527:  10%|█         | 112/1110 [00:56\u003c08:22,  1.99it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.817, acc: 0.555:  12%|█▏        | 128/1110 [01:04\u003c08:23,  1.95it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.779, acc: 0.576:  13%|█▎        | 144/1110 [01:12\u003c08:10,  1.97it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.764, acc: 0.587:  14%|█▍        | 160/1110 [01:21\u003c08:05,  1.96it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.755, acc: 0.585:  16%|█▌        | 176/1110 [01:28\u003c07:49,  1.99it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.751, acc: 0.583:  17%|█▋        | 192/1110 [01:36\u003c07:35,  2.02it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.745, acc: 0.582:  19%|█▊        | 208/1110 [01:44\u003c07:28,  2.01it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.743, acc: 0.585:  20%|██        | 224/1110 [01:51\u003c07:12,  2.05it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.730, acc: 0.596:  22%|██▏       | 240/1110 [02:00\u003c07:14,  2.00it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.748, acc: 0.590:  23%|██▎       | 256/1110 [02:04\u003c06:10,  2.30it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.742, acc: 0.592:  25%|██▍       | 272/1110 [02:08\u003c05:12,  2.68it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.748, acc: 0.590:  26%|██▌       | 288/1110 [02:12\u003c04:30,  3.04it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.748, acc: 0.589:  27%|██▋       | 304/1110 [02:15\u003c03:59,  3.37it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.750, acc: 0.584:  29%|██▉       | 320/1110 [02:19\u003c03:37,  3.63it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.749, acc: 0.580:  30%|███       | 336/1110 [02:22\u003c03:19,  3.87it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.749, acc: 0.571:  32%|███▏      | 352/1110 [02:26\u003c03:07,  4.04it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.756, acc: 0.565:  33%|███▎      | 368/1110 [02:29\u003c02:57,  4.18it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.753, acc: 0.562:  35%|███▍      | 384/1110 [02:33\u003c02:49,  4.29it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.747, acc: 0.570:  36%|███▌      | 400/1110 [02:36\u003c02:44,  4.33it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.741, acc: 0.577:  37%|███▋      | 416/1110 [02:40\u003c02:41,  4.28it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.737, acc: 0.574:  39%|███▉      | 432/1110 [02:44\u003c02:37,  4.30it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.732, acc: 0.576:  40%|████      | 448/1110 [02:48\u003c02:31,  4.36it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.735, acc: 0.569:  42%|████▏     | 464/1110 [02:51\u003c02:28,  4.36it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.741, acc: 0.562:  43%|████▎     | 480/1110 [02:55\u003c02:22,  4.42it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.739, acc: 0.569:  45%|████▍     | 496/1110 [02:58\u003c02:20,  4.37it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.742, acc: 0.564:  46%|████▌     | 512/1110 [03:02\u003c02:19,  4.30it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.743, acc: 0.566:  48%|████▊     | 528/1110 [03:06\u003c02:13,  4.37it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.746, acc: 0.570:  49%|████▉     | 544/1110 [03:09\u003c02:08,  4.39it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.750, acc: 0.566:  50%|█████     | 560/1110 [03:13\u003c02:05,  4.39it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.748, acc: 0.566:  52%|█████▏    | 576/1110 [03:17\u003c02:00,  4.43it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.747, acc: 0.568:  53%|█████▎    | 592/1110 [03:20\u003c01:57,  4.43it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.745, acc: 0.566:  55%|█████▍    | 608/1110 [03:24\u003c01:53,  4.42it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.745, acc: 0.569:  56%|█████▌    | 624/1110 [03:28\u003c01:51,  4.38it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.743, acc: 0.567:  58%|█████▊    | 640/1110 [03:32\u003c01:56,  4.03it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.742, acc: 0.562:  59%|█████▉    | 656/1110 [03:39\u003c02:11,  3.44it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.740, acc: 0.561:  61%|██████    | 672/1110 [03:44\u003c02:16,  3.20it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.742, acc: 0.554:  62%|██████▏   | 688/1110 [03:49\u003c02:12,  3.18it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.739, acc: 0.555:  63%|██████▎   | 704/1110 [03:53\u003c01:58,  3.41it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.735, acc: 0.560:  65%|██████▍   | 720/1110 [03:57\u003c01:48,  3.59it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.734, acc: 0.560:  66%|██████▋   | 736/1110 [04:01\u003c01:42,  3.65it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.732, acc: 0.562:  68%|██████▊   | 752/1110 [04:05\u003c01:35,  3.75it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.732, acc: 0.564:  69%|██████▉   | 768/1110 [04:10\u003c01:30,  3.79it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.732, acc: 0.562:  71%|███████   | 784/1110 [04:17\u003c01:45,  3.10it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.729, acc: 0.566:  72%|███████▏  | 800/1110 [04:23\u003c01:43,  3.01it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.727, acc: 0.569:  74%|███████▎  | 816/1110 [04:27\u003c01:34,  3.11it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.724, acc: 0.571:  75%|███████▍  | 832/1110 [04:32\u003c01:26,  3.20it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.758, acc: 0.568:  76%|███████▋  | 848/1110 [04:37\u003c01:20,  3.26it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.768, acc: 0.568:  78%|███████▊  | 864/1110 [04:42\u003c01:15,  3.27it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.764, acc: 0.572:  79%|███████▉  | 880/1110 [04:46\u003c01:10,  3.28it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.769, acc: 0.570:  81%|████████  | 896/1110 [04:51\u003c01:04,  3.29it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.773, acc: 0.570:  82%|████████▏ | 912/1110 [04:55\u003c00:55,  3.57it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.776, acc: 0.568:  84%|████████▎ | 928/1110 [04:59\u003c00:48,  3.78it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.775, acc: 0.572:  85%|████████▌ | 944/1110 [05:03\u003c00:43,  3.83it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.774, acc: 0.568:  86%|████████▋ | 960/1110 [05:06\u003c00:38,  3.94it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.773, acc: 0.566:  88%|████████▊ | 976/1110 [05:10\u003c00:33,  4.02it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.772, acc: 0.565:  89%|████████▉ | 992/1110 [05:14\u003c00:28,  4.07it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.770, acc: 0.563:  91%|█████████ | 1008/1110 [05:18\u003c00:24,  4.11it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.770, acc: 0.562:  92%|█████████▏| 1024/1110 [05:22\u003c00:20,  4.16it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.768, acc: 0.562:  94%|█████████▎| 1040/1110 [05:25\u003c00:16,  4.15it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.771, acc: 0.560:  95%|█████████▌| 1056/1110 [05:29\u003c00:13,  4.14it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.767, acc: 0.562:  97%|█████████▋| 1072/1110 [05:33\u003c00:09,  4.20it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.764, acc: 0.566:  98%|█████████▊| 1088/1110 [05:37\u003c00:05,  4.20it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.761, acc: 0.568:  99%|█████████▉| 1104/1110 [05:41\u003c00:01,  4.19it/s]",
            "\n",
            "\rEpoch: 2 - loss: 0.762, acc: 0.568, val_loss: 0.643, val_acc: 0.668: 100%|██████████| 1110/1110 [06:10\u003c00:00,  1.63s/it]",
            "\rTraining:   0%|          | 3/1000 [27:33\u003c156:49:45, 566.28s/it]",
            "\n",
            "\rEpoch: 3:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.610, acc: 0.750:   1%|▏         | 16/1110 [00:03\u003c04:25,  4.13it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.613, acc: 0.688:   3%|▎         | 32/1110 [00:07\u003c04:18,  4.16it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.627, acc: 0.625:   4%|▍         | 48/1110 [00:11\u003c04:10,  4.24it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.701, acc: 0.531:   6%|▌         | 64/1110 [00:14\u003c04:02,  4.30it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.687, acc: 0.562:   7%|▋         | 80/1110 [00:18\u003c04:02,  4.25it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.696, acc: 0.562:   9%|▊         | 96/1110 [00:22\u003c03:57,  4.28it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.767, acc: 0.554:  10%|█         | 112/1110 [00:26\u003c03:50,  4.32it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.802, acc: 0.562:  12%|█▏        | 128/1110 [00:29\u003c03:50,  4.27it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.791, acc: 0.562:  13%|█▎        | 144/1110 [00:33\u003c03:46,  4.26it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.775, acc: 0.569:  14%|█▍        | 160/1110 [00:37\u003c03:41,  4.30it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.761, acc: 0.580:  16%|█▌        | 176/1110 [00:41\u003c03:41,  4.22it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.781, acc: 0.578:  17%|█▋        | 192/1110 [00:44\u003c03:35,  4.26it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.853, acc: 0.562:  19%|█▊        | 208/1110 [00:48\u003c03:30,  4.29it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.872, acc: 0.567:  20%|██        | 224/1110 [00:52\u003c03:29,  4.23it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.862, acc: 0.554:  22%|██▏       | 240/1110 [00:56\u003c03:24,  4.25it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.863, acc: 0.555:  23%|██▎       | 256/1110 [00:59\u003c03:20,  4.26it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.922, acc: 0.548:  25%|██▍       | 272/1110 [01:03\u003c03:15,  4.29it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.940, acc: 0.545:  26%|██▌       | 288/1110 [01:07\u003c03:09,  4.33it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.968, acc: 0.543:  27%|██▋       | 304/1110 [01:11\u003c03:14,  4.14it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.958, acc: 0.541:  29%|██▉       | 320/1110 [01:15\u003c03:09,  4.17it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.947, acc: 0.536:  30%|███       | 336/1110 [01:18\u003c03:03,  4.22it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.937, acc: 0.537:  32%|███▏      | 352/1110 [01:22\u003c02:56,  4.28it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.953, acc: 0.535:  33%|███▎      | 368/1110 [01:26\u003c02:50,  4.36it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.952, acc: 0.534:  35%|███▍      | 384/1110 [01:29\u003c02:48,  4.31it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.951, acc: 0.527:  36%|███▌      | 400/1110 [01:33\u003c02:43,  4.35it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.939, acc: 0.534:  37%|███▋      | 416/1110 [01:37\u003c02:39,  4.36it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.927, acc: 0.544:  39%|███▉      | 432/1110 [01:40\u003c02:36,  4.34it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.922, acc: 0.542:  40%|████      | 448/1110 [01:44\u003c02:30,  4.39it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.905, acc: 0.552:  42%|████▏     | 464/1110 [01:48\u003c02:26,  4.40it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.931, acc: 0.548:  43%|████▎     | 480/1110 [01:51\u003c02:26,  4.29it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.930, acc: 0.546:  45%|████▍     | 496/1110 [01:55\u003c02:20,  4.36it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.924, acc: 0.547:  46%|████▌     | 512/1110 [01:59\u003c02:19,  4.30it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.928, acc: 0.542:  48%|████▊     | 528/1110 [02:02\u003c02:13,  4.37it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.922, acc: 0.544:  49%|████▉     | 544/1110 [02:06\u003c02:08,  4.40it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.919, acc: 0.539:  50%|█████     | 560/1110 [02:10\u003c02:05,  4.39it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.911, acc: 0.542:  52%|█████▏    | 576/1110 [02:13\u003c02:03,  4.34it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.907, acc: 0.544:  53%|█████▎    | 592/1110 [02:17\u003c02:00,  4.30it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.898, acc: 0.549:  55%|█████▍    | 608/1110 [02:21\u003c01:58,  4.23it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.893, acc: 0.551:  56%|█████▌    | 624/1110 [02:25\u003c01:53,  4.29it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.889, acc: 0.552:  58%|█████▊    | 640/1110 [02:29\u003c01:50,  4.25it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.880, acc: 0.556:  59%|█████▉    | 656/1110 [02:32\u003c01:47,  4.24it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.877, acc: 0.554:  61%|██████    | 672/1110 [02:36\u003c01:43,  4.24it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.878, acc: 0.547:  62%|██████▏   | 688/1110 [02:40\u003c01:39,  4.22it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.872, acc: 0.548:  63%|██████▎   | 704/1110 [02:44\u003c01:35,  4.25it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.866, acc: 0.551:  65%|██████▍   | 720/1110 [02:47\u003c01:31,  4.25it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.861, acc: 0.553:  66%|██████▋   | 736/1110 [02:51\u003c01:28,  4.24it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.857, acc: 0.555:  68%|██████▊   | 752/1110 [02:56\u003c01:31,  3.90it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.852, acc: 0.557:  69%|██████▉   | 768/1110 [03:04\u003c01:48,  3.14it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.848, acc: 0.557:  71%|███████   | 784/1110 [03:12\u003c02:06,  2.58it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.845, acc: 0.558:  72%|███████▏  | 800/1110 [03:20\u003c02:07,  2.43it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.841, acc: 0.559:  74%|███████▎  | 816/1110 [03:28\u003c02:08,  2.30it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.838, acc: 0.561:  75%|███████▍  | 832/1110 [03:36\u003c02:06,  2.20it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.835, acc: 0.560:  76%|███████▋  | 848/1110 [03:43\u003c01:57,  2.22it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.833, acc: 0.560:  78%|███████▊  | 864/1110 [03:50\u003c01:52,  2.18it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.830, acc: 0.561:  79%|███████▉  | 880/1110 [03:58\u003c01:48,  2.12it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.827, acc: 0.562:  81%|████████  | 896/1110 [04:05\u003c01:39,  2.15it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.824, acc: 0.564:  82%|████████▏ | 912/1110 [04:12\u003c01:29,  2.21it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.822, acc: 0.562:  84%|████████▎ | 928/1110 [04:20\u003c01:23,  2.18it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.821, acc: 0.562:  85%|████████▌ | 944/1110 [04:27\u003c01:16,  2.18it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.818, acc: 0.564:  86%|████████▋ | 960/1110 [04:35\u003c01:09,  2.16it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.815, acc: 0.565:  88%|████████▊ | 976/1110 [04:42\u003c01:02,  2.14it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.816, acc: 0.557:  89%|████████▉ | 992/1110 [04:50\u003c00:56,  2.10it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.813, acc: 0.560:  91%|█████████ | 1008/1110 [04:58\u003c00:48,  2.11it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.810, acc: 0.561:  92%|█████████▏| 1024/1110 [05:06\u003c00:40,  2.10it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.808, acc: 0.562:  94%|█████████▎| 1040/1110 [05:13\u003c00:33,  2.10it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.807, acc: 0.562:  95%|█████████▌| 1056/1110 [05:21\u003c00:25,  2.09it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.802, acc: 0.566:  97%|█████████▋| 1072/1110 [05:29\u003c00:18,  2.02it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.798, acc: 0.571:  98%|█████████▊| 1088/1110 [05:38\u003c00:11,  1.99it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.798, acc: 0.571:  99%|█████████▉| 1104/1110 [05:48\u003c00:03,  1.85it/s]",
            "\n",
            "\rEpoch: 3 - loss: 0.797, acc: 0.571, val_loss: 0.801, val_acc: 0.536: 100%|██████████| 1110/1110 [06:47\u003c00:00,  3.31s/it]",
            "\rTraining:   0%|          | 4/1000 [34:20\u003c143:27:08, 518.50s/it]",
            "\n",
            "\rEpoch: 4:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 4 - loss: 1.190, acc: 0.375:   1%|▏         | 16/1110 [00:08\u003c10:10,  1.79it/s]",
            "\n",
            "\rEpoch: 4 - loss: 1.002, acc: 0.375:   3%|▎         | 32/1110 [00:16\u003c09:43,  1.85it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.899, acc: 0.417:   4%|▍         | 48/1110 [00:25\u003c09:22,  1.89it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.823, acc: 0.484:   6%|▌         | 64/1110 [00:32\u003c09:02,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.848, acc: 0.488:   7%|▋         | 80/1110 [00:41\u003c08:52,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.844, acc: 0.510:   9%|▊         | 96/1110 [00:48\u003c08:27,  2.00it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.844, acc: 0.500:  10%|█         | 112/1110 [00:57\u003c08:30,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.819, acc: 0.516:  12%|█▏        | 128/1110 [01:05\u003c08:17,  1.97it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.808, acc: 0.514:  13%|█▎        | 144/1110 [01:13\u003c08:20,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.804, acc: 0.506:  14%|█▍        | 160/1110 [01:22\u003c08:25,  1.88it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.792, acc: 0.511:  16%|█▌        | 176/1110 [01:31\u003c08:13,  1.89it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.781, acc: 0.521:  17%|█▋        | 192/1110 [01:39\u003c07:56,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.766, acc: 0.543:  19%|█▊        | 208/1110 [01:46\u003c07:36,  1.97it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.748, acc: 0.562:  20%|██        | 224/1110 [01:55\u003c07:33,  1.95it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.772, acc: 0.567:  22%|██▏       | 240/1110 [02:03\u003c07:25,  1.95it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.761, acc: 0.574:  23%|██▎       | 256/1110 [02:11\u003c07:16,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.763, acc: 0.574:  25%|██▍       | 272/1110 [02:18\u003c06:56,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.757, acc: 0.569:  26%|██▌       | 288/1110 [02:26\u003c06:42,  2.04it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.752, acc: 0.572:  27%|██▋       | 304/1110 [02:33\u003c06:25,  2.09it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.764, acc: 0.562:  29%|██▉       | 320/1110 [02:41\u003c06:19,  2.08it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.768, acc: 0.554:  30%|███       | 336/1110 [02:49\u003c06:10,  2.09it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.762, acc: 0.554:  32%|███▏      | 352/1110 [02:56\u003c06:01,  2.10it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.761, acc: 0.552:  33%|███▎      | 368/1110 [03:04\u003c06:03,  2.04it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.760, acc: 0.549:  35%|███▍      | 384/1110 [03:12\u003c05:58,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.751, acc: 0.562:  36%|███▌      | 400/1110 [03:20\u003c05:50,  2.03it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.751, acc: 0.560:  37%|███▋      | 416/1110 [03:28\u003c05:37,  2.06it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.749, acc: 0.560:  39%|███▉      | 432/1110 [03:36\u003c05:35,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.743, acc: 0.562:  40%|████      | 448/1110 [03:44\u003c05:33,  1.99it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.741, acc: 0.562:  42%|████▏     | 464/1110 [03:52\u003c05:19,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.745, acc: 0.554:  43%|████▎     | 480/1110 [04:00\u003c05:16,  1.99it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.740, acc: 0.562:  45%|████▍     | 496/1110 [04:08\u003c05:05,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.736, acc: 0.564:  46%|████▌     | 512/1110 [04:16\u003c05:01,  1.98it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.737, acc: 0.564:  48%|████▊     | 528/1110 [04:24\u003c04:48,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.734, acc: 0.568:  49%|████▉     | 544/1110 [04:32\u003c04:45,  1.98it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.737, acc: 0.562:  50%|█████     | 560/1110 [04:40\u003c04:33,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.739, acc: 0.564:  52%|█████▏    | 576/1110 [04:49\u003c04:32,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.737, acc: 0.564:  53%|█████▎    | 592/1110 [04:56\u003c04:18,  2.00it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.735, acc: 0.566:  55%|█████▍    | 608/1110 [05:05\u003c04:14,  1.97it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.733, acc: 0.564:  56%|█████▌    | 624/1110 [05:13\u003c04:07,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.732, acc: 0.566:  58%|█████▊    | 640/1110 [05:21\u003c03:55,  2.00it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.731, acc: 0.567:  59%|█████▉    | 656/1110 [05:29\u003c03:45,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.729, acc: 0.570:  61%|██████    | 672/1110 [05:36\u003c03:36,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.726, acc: 0.574:  62%|██████▏   | 688/1110 [05:43\u003c03:22,  2.09it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.723, acc: 0.578:  63%|██████▎   | 704/1110 [05:51\u003c03:16,  2.07it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.718, acc: 0.585:  65%|██████▍   | 720/1110 [05:59\u003c03:11,  2.04it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.715, acc: 0.588:  66%|██████▋   | 736/1110 [06:07\u003c03:02,  2.05it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.711, acc: 0.593:  68%|██████▊   | 752/1110 [06:15\u003c02:55,  2.04it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.708, acc: 0.595:  69%|██████▉   | 768/1110 [06:24\u003c02:51,  1.99it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.705, acc: 0.597:  71%|███████   | 784/1110 [06:31\u003c02:42,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.710, acc: 0.595:  72%|███████▏  | 800/1110 [06:39\u003c02:35,  2.00it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.716, acc: 0.593:  74%|███████▎  | 816/1110 [06:48\u003c02:29,  1.97it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.716, acc: 0.593:  75%|███████▍  | 832/1110 [06:56\u003c02:22,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.724, acc: 0.590:  76%|███████▋  | 848/1110 [07:05\u003c02:15,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.740, acc: 0.588:  78%|███████▊  | 864/1110 [07:12\u003c02:02,  2.02it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.744, acc: 0.586:  79%|███████▉  | 880/1110 [07:20\u003c01:56,  1.98it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.745, acc: 0.586:  81%|████████  | 896/1110 [07:28\u003c01:47,  1.99it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.743, acc: 0.589:  82%|████████▏ | 912/1110 [07:37\u003c01:40,  1.97it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.745, acc: 0.587:  84%|████████▎ | 928/1110 [07:44\u003c01:30,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.743, acc: 0.589:  85%|████████▌ | 944/1110 [07:52\u003c01:21,  2.05it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.760, acc: 0.590:  86%|████████▋ | 960/1110 [07:59\u003c01:12,  2.07it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.768, acc: 0.586:  88%|████████▊ | 976/1110 [08:07\u003c01:04,  2.09it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.768, acc: 0.586:  89%|████████▉ | 992/1110 [08:15\u003c00:58,  2.01it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.770, acc: 0.581:  91%|█████████ | 1008/1110 [08:23\u003c00:51,  1.99it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.773, acc: 0.579:  92%|█████████▏| 1024/1110 [08:32\u003c00:43,  1.96it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.775, acc: 0.578:  94%|█████████▎| 1040/1110 [08:40\u003c00:35,  1.95it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.773, acc: 0.578:  95%|█████████▌| 1056/1110 [08:49\u003c00:28,  1.92it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.770, acc: 0.579:  97%|█████████▋| 1072/1110 [08:58\u003c00:20,  1.87it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.774, acc: 0.578:  98%|█████████▊| 1088/1110 [09:06\u003c00:11,  1.92it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.774, acc: 0.579:  99%|█████████▉| 1104/1110 [09:14\u003c00:03,  1.93it/s]",
            "\n",
            "\rEpoch: 4 - loss: 0.773, acc: 0.580, val_loss: 0.615, val_acc: 0.668: 100%|██████████| 1110/1110 [10:17\u003c00:00,  3.52s/it]",
            "\rTraining:   0%|          | 5/1000 [44:38\u003c151:31:03, 548.20s/it]",
            "\n",
            "\rEpoch: 5:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.618, acc: 0.562:   1%|▏         | 16/1110 [00:07\u003c08:46,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.610, acc: 0.656:   3%|▎         | 32/1110 [00:14\u003c08:26,  2.13it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.604, acc: 0.688:   4%|▍         | 48/1110 [00:22\u003c08:15,  2.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.595, acc: 0.703:   6%|▌         | 64/1110 [00:30\u003c08:16,  2.11it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.663, acc: 0.688:   7%|▋         | 80/1110 [00:37\u003c08:06,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.691, acc: 0.667:   9%|▊         | 96/1110 [00:45\u003c08:06,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.680, acc: 0.661:  10%|█         | 112/1110 [00:53\u003c08:00,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.705, acc: 0.625:  12%|█▏        | 128/1110 [01:00\u003c07:47,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.695, acc: 0.618:  13%|█▎        | 144/1110 [01:07\u003c07:31,  2.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.678, acc: 0.631:  14%|█▍        | 160/1110 [01:14\u003c07:18,  2.17it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.669, acc: 0.642:  16%|█▌        | 176/1110 [01:22\u003c07:07,  2.18it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.688, acc: 0.646:  17%|█▋        | 192/1110 [01:30\u003c07:14,  2.11it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.675, acc: 0.649:  19%|█▊        | 208/1110 [01:37\u003c07:03,  2.13it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.669, acc: 0.656:  20%|██        | 224/1110 [01:45\u003c06:57,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.648, acc: 0.671:  22%|██▏       | 240/1110 [01:53\u003c06:58,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.675, acc: 0.664:  23%|██▎       | 256/1110 [02:00\u003c06:48,  2.09it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.690, acc: 0.651:  25%|██▍       | 272/1110 [02:08\u003c06:39,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.683, acc: 0.656:  26%|██▌       | 288/1110 [02:15\u003c06:28,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.694, acc: 0.645:  27%|██▋       | 304/1110 [02:22\u003c06:12,  2.16it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.691, acc: 0.644:  29%|██▉       | 320/1110 [02:30\u003c06:07,  2.15it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.692, acc: 0.640:  30%|███       | 336/1110 [02:37\u003c06:00,  2.15it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.692, acc: 0.631:  32%|███▏      | 352/1110 [02:45\u003c05:50,  2.16it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.686, acc: 0.639:  33%|███▎      | 368/1110 [02:53\u003c05:53,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.693, acc: 0.635:  35%|███▍      | 384/1110 [03:00\u003c05:41,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.693, acc: 0.635:  36%|███▌      | 400/1110 [03:07\u003c05:31,  2.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.703, acc: 0.625:  37%|███▋      | 416/1110 [03:15\u003c05:24,  2.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.702, acc: 0.623:  39%|███▉      | 432/1110 [03:22\u003c05:16,  2.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.704, acc: 0.616:  40%|████      | 448/1110 [03:30\u003c05:12,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.709, acc: 0.608:  42%|████▏     | 464/1110 [03:38\u003c05:08,  2.09it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.704, acc: 0.615:  43%|████▎     | 480/1110 [03:46\u003c05:02,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.701, acc: 0.617:  45%|████▍     | 496/1110 [03:53\u003c04:53,  2.09it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.708, acc: 0.611:  46%|████▌     | 512/1110 [04:01\u003c04:49,  2.06it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.704, acc: 0.614:  48%|████▊     | 528/1110 [04:09\u003c04:44,  2.04it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.699, acc: 0.619:  49%|████▉     | 544/1110 [04:17\u003c04:32,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.698, acc: 0.620:  50%|█████     | 560/1110 [04:24\u003c04:21,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.697, acc: 0.620:  52%|█████▏    | 576/1110 [04:32\u003c04:15,  2.09it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.700, acc: 0.611:  53%|█████▎    | 592/1110 [04:40\u003c04:08,  2.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.698, acc: 0.613:  55%|█████▍    | 608/1110 [04:47\u003c03:56,  2.12it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.698, acc: 0.612:  56%|█████▌    | 624/1110 [04:55\u003c03:51,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.695, acc: 0.613:  58%|█████▊    | 640/1110 [05:02\u003c03:43,  2.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.696, acc: 0.611:  59%|█████▉    | 656/1110 [05:10\u003c03:41,  2.05it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.696, acc: 0.609:  61%|██████    | 672/1110 [05:15\u003c03:09,  2.32it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.695, acc: 0.608:  62%|██████▏   | 688/1110 [05:20\u003c02:47,  2.52it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.695, acc: 0.608:  63%|██████▎   | 704/1110 [05:26\u003c02:33,  2.65it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.693, acc: 0.610:  65%|██████▍   | 720/1110 [05:30\u003c02:16,  2.87it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.696, acc: 0.607:  66%|██████▋   | 736/1110 [05:35\u003c02:03,  3.04it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.693, acc: 0.610:  68%|██████▊   | 752/1110 [05:39\u003c01:51,  3.22it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.694, acc: 0.608:  69%|██████▉   | 768/1110 [05:43\u003c01:42,  3.33it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.693, acc: 0.608:  71%|███████   | 784/1110 [05:48\u003c01:35,  3.40it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.692, acc: 0.609:  72%|███████▏  | 800/1110 [05:52\u003c01:27,  3.56it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.702, acc: 0.607:  74%|███████▎  | 816/1110 [05:56\u003c01:18,  3.73it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.710, acc: 0.605:  75%|███████▍  | 832/1110 [06:00\u003c01:12,  3.85it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.706, acc: 0.607:  76%|███████▋  | 848/1110 [06:04\u003c01:07,  3.87it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.708, acc: 0.608:  78%|███████▊  | 864/1110 [06:07\u003c01:01,  3.99it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.708, acc: 0.611:  79%|███████▉  | 880/1110 [06:11\u003c00:56,  4.06it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.708, acc: 0.608:  81%|████████  | 896/1110 [06:15\u003c00:52,  4.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.710, acc: 0.604:  82%|████████▏ | 912/1110 [06:19\u003c00:47,  4.15it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.711, acc: 0.602:  84%|████████▎ | 928/1110 [06:22\u003c00:43,  4.19it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.711, acc: 0.600:  85%|████████▌ | 944/1110 [06:26\u003c00:39,  4.22it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.709, acc: 0.602:  86%|████████▋ | 960/1110 [06:30\u003c00:36,  4.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.709, acc: 0.603:  88%|████████▊ | 976/1110 [06:34\u003c00:32,  4.10it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.711, acc: 0.602:  89%|████████▉ | 992/1110 [06:38\u003c00:28,  4.07it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.709, acc: 0.605:  91%|█████████ | 1008/1110 [06:42\u003c00:24,  4.09it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.708, acc: 0.604:  92%|█████████▏| 1024/1110 [06:46\u003c00:21,  4.05it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.707, acc: 0.607:  94%|█████████▎| 1040/1110 [06:50\u003c00:17,  4.08it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.707, acc: 0.605:  95%|█████████▌| 1056/1110 [06:54\u003c00:13,  4.07it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.705, acc: 0.606:  97%|█████████▋| 1072/1110 [06:58\u003c00:09,  4.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.704, acc: 0.606:  98%|█████████▊| 1088/1110 [07:01\u003c00:05,  4.19it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.704, acc: 0.604:  99%|█████████▉| 1104/1110 [07:05\u003c00:01,  4.14it/s]",
            "\n",
            "\rEpoch: 5 - loss: 0.703, acc: 0.605, val_loss: 0.644, val_acc: 0.601: 100%|██████████| 1110/1110 [07:36\u003c00:00,  1.70s/it]",
            "\rTraining:   1%|          | 6/1000 [52:14\u003c143:45:25, 520.65s/it]",
            "\n",
            "\rEpoch: 6:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.723, acc: 0.438:   1%|▏         | 16/1110 [00:04\u003c04:39,  3.91it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.698, acc: 0.594:   3%|▎         | 32/1110 [00:08\u003c04:32,  3.95it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.720, acc: 0.542:   4%|▍         | 48/1110 [00:11\u003c04:26,  3.99it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.773, acc: 0.531:   6%|▌         | 64/1110 [00:16\u003c04:27,  3.90it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.735, acc: 0.562:   7%|▋         | 80/1110 [00:20\u003c04:19,  3.96it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.711, acc: 0.562:   9%|▊         | 96/1110 [00:24\u003c04:26,  3.80it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.697, acc: 0.598:  10%|█         | 112/1110 [00:29\u003c04:27,  3.72it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.694, acc: 0.602:  12%|█▏        | 128/1110 [00:33\u003c04:20,  3.77it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.695, acc: 0.604:  13%|█▎        | 144/1110 [00:37\u003c04:05,  3.93it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.691, acc: 0.619:  14%|█▍        | 160/1110 [00:40\u003c03:57,  3.99it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.678, acc: 0.625:  16%|█▌        | 176/1110 [00:44\u003c03:51,  4.04it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.670, acc: 0.630:  17%|█▋        | 192/1110 [00:49\u003c04:02,  3.79it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.669, acc: 0.625:  19%|█▊        | 208/1110 [00:54\u003c04:06,  3.66it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.666, acc: 0.629:  20%|██        | 224/1110 [00:58\u003c04:06,  3.60it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.662, acc: 0.633:  22%|██▏       | 240/1110 [01:03\u003c04:02,  3.59it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.660, acc: 0.625:  23%|██▎       | 256/1110 [01:08\u003c04:02,  3.52it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.659, acc: 0.621:  25%|██▍       | 272/1110 [01:12\u003c03:59,  3.51it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.667, acc: 0.618:  26%|██▌       | 288/1110 [01:17\u003c03:58,  3.44it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.668, acc: 0.612:  27%|██▋       | 304/1110 [01:22\u003c03:52,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.667, acc: 0.609:  29%|██▉       | 320/1110 [01:26\u003c03:40,  3.59it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.668, acc: 0.607:  30%|███       | 336/1110 [01:30\u003c03:39,  3.52it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.661, acc: 0.611:  32%|███▏      | 352/1110 [01:35\u003c03:37,  3.48it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.656, acc: 0.617:  33%|███▎      | 368/1110 [01:40\u003c03:35,  3.44it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.656, acc: 0.617:  35%|███▍      | 384/1110 [01:45\u003c03:29,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.656, acc: 0.620:  36%|███▌      | 400/1110 [01:49\u003c03:28,  3.41it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.655, acc: 0.623:  37%|███▋      | 416/1110 [01:54\u003c03:24,  3.39it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.659, acc: 0.618:  39%|███▉      | 432/1110 [01:59\u003c03:18,  3.42it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.662, acc: 0.614:  40%|████      | 448/1110 [02:04\u003c03:15,  3.38it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.685, acc: 0.603:  42%|████▏     | 464/1110 [02:08\u003c03:11,  3.38it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.688, acc: 0.608:  43%|████▎     | 480/1110 [02:13\u003c03:08,  3.34it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.708, acc: 0.599:  45%|████▍     | 496/1110 [02:18\u003c03:03,  3.35it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.712, acc: 0.596:  46%|████▌     | 512/1110 [02:23\u003c02:56,  3.40it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.712, acc: 0.597:  48%|████▊     | 528/1110 [02:27\u003c02:52,  3.38it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.713, acc: 0.597:  49%|████▉     | 544/1110 [02:32\u003c02:45,  3.42it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.707, acc: 0.605:  50%|█████     | 560/1110 [02:37\u003c02:42,  3.37it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.702, acc: 0.608:  52%|█████▏    | 576/1110 [02:41\u003c02:35,  3.43it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.698, acc: 0.615:  53%|█████▎    | 592/1110 [02:46\u003c02:29,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.699, acc: 0.615:  55%|█████▍    | 608/1110 [02:50\u003c02:24,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.699, acc: 0.615:  56%|█████▌    | 624/1110 [02:55\u003c02:20,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.695, acc: 0.619:  58%|█████▊    | 640/1110 [02:59\u003c02:14,  3.50it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.694, acc: 0.619:  59%|█████▉    | 656/1110 [03:04\u003c02:09,  3.50it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.689, acc: 0.624:  61%|██████    | 672/1110 [03:09\u003c02:06,  3.45it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.693, acc: 0.619:  62%|██████▏   | 688/1110 [03:13\u003c02:00,  3.50it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.691, acc: 0.621:  63%|██████▎   | 704/1110 [03:18\u003c01:56,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.687, acc: 0.624:  65%|██████▍   | 720/1110 [03:23\u003c01:52,  3.48it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.691, acc: 0.622:  66%|██████▋   | 736/1110 [03:27\u003c01:45,  3.53it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.698, acc: 0.620:  68%|██████▊   | 752/1110 [03:32\u003c01:42,  3.48it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.696, acc: 0.618:  69%|██████▉   | 768/1110 [03:36\u003c01:38,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.694, acc: 0.619:  71%|███████   | 784/1110 [03:41\u003c01:33,  3.49it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.691, acc: 0.620:  72%|███████▏  | 800/1110 [03:45\u003c01:29,  3.48it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.698, acc: 0.616:  74%|███████▎  | 816/1110 [03:50\u003c01:26,  3.41it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.703, acc: 0.613:  75%|███████▍  | 832/1110 [03:55\u003c01:20,  3.44it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.705, acc: 0.611:  76%|███████▋  | 848/1110 [04:00\u003c01:15,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.703, acc: 0.610:  78%|███████▊  | 864/1110 [04:04\u003c01:12,  3.41it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.705, acc: 0.610:  79%|███████▉  | 880/1110 [04:09\u003c01:06,  3.44it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.706, acc: 0.613:  81%|████████  | 896/1110 [04:13\u003c01:01,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.708, acc: 0.612:  82%|████████▏ | 912/1110 [04:18\u003c00:57,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.706, acc: 0.612:  84%|████████▎ | 928/1110 [04:23\u003c00:52,  3.49it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.706, acc: 0.612:  85%|████████▌ | 944/1110 [04:27\u003c00:47,  3.53it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.717, acc: 0.608:  86%|████████▋ | 960/1110 [04:32\u003c00:42,  3.52it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.716, acc: 0.608:  88%|████████▊ | 976/1110 [04:36\u003c00:38,  3.51it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.714, acc: 0.611:  89%|████████▉ | 992/1110 [04:41\u003c00:33,  3.51it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.714, acc: 0.611:  91%|█████████ | 1008/1110 [04:45\u003c00:29,  3.49it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.712, acc: 0.613:  92%|█████████▏| 1024/1110 [04:50\u003c00:24,  3.48it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.718, acc: 0.612:  94%|█████████▎| 1040/1110 [04:55\u003c00:20,  3.47it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.720, acc: 0.612:  95%|█████████▌| 1056/1110 [04:59\u003c00:15,  3.51it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.721, acc: 0.610:  97%|█████████▋| 1072/1110 [05:04\u003c00:10,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.721, acc: 0.608:  98%|█████████▊| 1088/1110 [05:08\u003c00:06,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.724, acc: 0.606:  99%|█████████▉| 1104/1110 [05:13\u003c00:01,  3.46it/s]",
            "\n",
            "\rEpoch: 6 - loss: 0.724, acc: 0.606, val_loss: 0.648, val_acc: 0.663: 100%|██████████| 1110/1110 [05:50\u003c00:00,  2.07s/it]",
            "\rTraining:   1%|          | 7/1000 [58:05\u003c129:34:14, 469.74s/it]",
            "\n",
            "\rEpoch: 7:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.737, acc: 0.625:   1%|▏         | 16/1110 [00:04\u003c05:22,  3.39it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.747, acc: 0.688:   3%|▎         | 32/1110 [00:09\u003c05:16,  3.41it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.672, acc: 0.729:   4%|▍         | 48/1110 [00:14\u003c05:14,  3.38it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.737, acc: 0.719:   6%|▌         | 64/1110 [00:19\u003c05:13,  3.33it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.726, acc: 0.688:   7%|▋         | 80/1110 [00:23\u003c05:08,  3.34it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.737, acc: 0.635:   9%|▊         | 96/1110 [00:28\u003c05:06,  3.31it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.725, acc: 0.625:  10%|█         | 112/1110 [00:33\u003c05:02,  3.30it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.730, acc: 0.594:  12%|█▏        | 128/1110 [00:38\u003c04:55,  3.32it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.711, acc: 0.597:  13%|█▎        | 144/1110 [00:43\u003c04:47,  3.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.714, acc: 0.594:  14%|█▍        | 160/1110 [00:47\u003c04:42,  3.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.698, acc: 0.602:  16%|█▌        | 176/1110 [00:52\u003c04:35,  3.39it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.687, acc: 0.615:  17%|█▋        | 192/1110 [00:56\u003c04:26,  3.44it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.685, acc: 0.625:  19%|█▊        | 208/1110 [01:01\u003c04:27,  3.37it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.686, acc: 0.625:  20%|██        | 224/1110 [01:06\u003c04:25,  3.34it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.705, acc: 0.621:  22%|██▏       | 240/1110 [01:11\u003c04:17,  3.38it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.708, acc: 0.629:  23%|██▎       | 256/1110 [01:16\u003c04:13,  3.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.700, acc: 0.636:  25%|██▍       | 272/1110 [01:20\u003c04:06,  3.40it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.691, acc: 0.639:  26%|██▌       | 288/1110 [01:25\u003c04:00,  3.42it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.703, acc: 0.628:  27%|██▋       | 304/1110 [01:29\u003c03:52,  3.47it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.712, acc: 0.613:  29%|██▉       | 320/1110 [01:34\u003c03:46,  3.48it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.708, acc: 0.613:  30%|███       | 336/1110 [01:38\u003c03:41,  3.50it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.715, acc: 0.608:  32%|███▏      | 352/1110 [01:43\u003c03:40,  3.44it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.745, acc: 0.603:  33%|███▎      | 368/1110 [01:48\u003c03:33,  3.47it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.751, acc: 0.604:  35%|███▍      | 384/1110 [01:52\u003c03:29,  3.47it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.747, acc: 0.605:  36%|███▌      | 400/1110 [01:57\u003c03:23,  3.50it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.742, acc: 0.601:  37%|███▋      | 416/1110 [02:02\u003c03:25,  3.37it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.745, acc: 0.597:  39%|███▉      | 432/1110 [02:07\u003c03:27,  3.27it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.738, acc: 0.603:  40%|████      | 448/1110 [02:12\u003c03:23,  3.25it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.739, acc: 0.603:  42%|████▏     | 464/1110 [02:17\u003c03:19,  3.24it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.744, acc: 0.596:  43%|████▎     | 480/1110 [02:22\u003c03:10,  3.31it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.746, acc: 0.593:  45%|████▍     | 496/1110 [02:27\u003c03:11,  3.20it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.744, acc: 0.592:  46%|████▌     | 512/1110 [02:32\u003c03:07,  3.18it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.741, acc: 0.595:  48%|████▊     | 528/1110 [02:38\u003c03:07,  3.10it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.755, acc: 0.596:  49%|████▉     | 544/1110 [02:42\u003c02:48,  3.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.752, acc: 0.598:  50%|█████     | 560/1110 [02:45\u003c02:30,  3.65it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.768, acc: 0.594:  52%|█████▏    | 576/1110 [02:49\u003c02:19,  3.84it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.771, acc: 0.591:  53%|█████▎    | 592/1110 [02:52\u003c02:09,  3.99it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.767, acc: 0.590:  55%|█████▍    | 608/1110 [02:56\u003c02:03,  4.07it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.766, acc: 0.588:  56%|█████▌    | 624/1110 [03:00\u003c01:56,  4.16it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.765, acc: 0.592:  58%|█████▊    | 640/1110 [03:04\u003c01:51,  4.20it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.762, acc: 0.595:  59%|█████▉    | 656/1110 [03:07\u003c01:46,  4.27it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.762, acc: 0.594:  61%|██████    | 672/1110 [03:11\u003c01:40,  4.35it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.762, acc: 0.594:  62%|██████▏   | 688/1110 [03:14\u003c01:36,  4.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.756, acc: 0.597:  63%|██████▎   | 704/1110 [03:18\u003c01:33,  4.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.752, acc: 0.600:  65%|██████▍   | 720/1110 [03:22\u003c01:28,  4.41it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.756, acc: 0.599:  66%|██████▋   | 736/1110 [03:25\u003c01:24,  4.40it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.753, acc: 0.602:  68%|██████▊   | 752/1110 [03:29\u003c01:20,  4.42it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.749, acc: 0.605:  69%|██████▉   | 768/1110 [03:33\u003c01:18,  4.33it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.747, acc: 0.605:  71%|███████   | 784/1110 [03:36\u003c01:13,  4.46it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.742, acc: 0.609:  72%|███████▏  | 800/1110 [03:40\u003c01:09,  4.45it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.735, acc: 0.613:  74%|███████▎  | 816/1110 [03:43\u003c01:07,  4.37it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.731, acc: 0.613:  75%|███████▍  | 832/1110 [03:47\u003c01:02,  4.43it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.729, acc: 0.613:  76%|███████▋  | 848/1110 [03:50\u003c00:58,  4.45it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.729, acc: 0.611:  78%|███████▊  | 864/1110 [03:54\u003c00:55,  4.46it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.726, acc: 0.614:  79%|███████▉  | 880/1110 [03:58\u003c00:52,  4.38it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.724, acc: 0.614:  81%|████████  | 896/1110 [04:02\u003c00:49,  4.36it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.734, acc: 0.609:  82%|████████▏ | 912/1110 [04:05\u003c00:44,  4.41it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.744, acc: 0.601:  84%|████████▎ | 928/1110 [04:09\u003c00:41,  4.41it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.742, acc: 0.602:  85%|████████▌ | 944/1110 [04:12\u003c00:37,  4.37it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.740, acc: 0.603:  86%|████████▋ | 960/1110 [04:16\u003c00:34,  4.40it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.755, acc: 0.601:  88%|████████▊ | 976/1110 [04:20\u003c00:30,  4.43it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.758, acc: 0.604:  89%|████████▉ | 992/1110 [04:23\u003c00:27,  4.37it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.763, acc: 0.605:  91%|█████████ | 1008/1110 [04:27\u003c00:22,  4.44it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.761, acc: 0.606:  92%|█████████▏| 1024/1110 [04:31\u003c00:19,  4.33it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.763, acc: 0.605:  94%|█████████▎| 1040/1110 [04:34\u003c00:16,  4.34it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.772, acc: 0.601:  95%|█████████▌| 1056/1110 [04:38\u003c00:12,  4.32it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.771, acc: 0.599:  97%|█████████▋| 1072/1110 [04:42\u003c00:08,  4.31it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.768, acc: 0.602:  98%|█████████▊| 1088/1110 [04:46\u003c00:05,  4.32it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.769, acc: 0.601:  99%|█████████▉| 1104/1110 [04:49\u003c00:01,  4.38it/s]",
            "\n",
            "\rEpoch: 7 - loss: 0.770, acc: 0.601, val_loss: 0.688, val_acc: 0.644: 100%|██████████| 1110/1110 [05:17\u003c00:00,  1.58s/it]",
            "\rTraining:   1%|          | 8/1000 [1:03:23\u003c116:53:25, 424.20s/it]",
            "\n",
            "\rEpoch: 8:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.867, acc: 0.562:   1%|▏         | 16/1110 [00:03\u003c04:26,  4.10it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.719:   3%|▎         | 32/1110 [00:07\u003c04:14,  4.23it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.663, acc: 0.688:   4%|▍         | 48/1110 [00:11\u003c04:10,  4.24it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.642, acc: 0.719:   6%|▌         | 64/1110 [00:14\u003c04:02,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.669, acc: 0.712:   7%|▋         | 80/1110 [00:18\u003c03:56,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.665, acc: 0.677:   9%|▊         | 96/1110 [00:22\u003c03:57,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.649, acc: 0.670:  10%|█         | 112/1110 [00:25\u003c03:53,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.633, acc: 0.680:  12%|█▏        | 128/1110 [00:29\u003c03:47,  4.32it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.641, acc: 0.674:  13%|█▎        | 144/1110 [00:33\u003c03:43,  4.32it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.652, acc: 0.669:  14%|█▍        | 160/1110 [00:37\u003c03:40,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.670:  16%|█▌        | 176/1110 [00:40\u003c03:38,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.641, acc: 0.672:  17%|█▋        | 192/1110 [00:44\u003c03:32,  4.33it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.647, acc: 0.659:  19%|█▊        | 208/1110 [00:48\u003c03:29,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.648, acc: 0.647:  20%|██        | 224/1110 [00:51\u003c03:23,  4.36it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.646:  22%|██▏       | 240/1110 [00:55\u003c03:19,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.649, acc: 0.652:  23%|██▎       | 256/1110 [00:59\u003c03:16,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.654:  25%|██▍       | 272/1110 [01:02\u003c03:12,  4.34it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.653:  26%|██▌       | 288/1110 [01:06\u003c03:10,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.646, acc: 0.655:  27%|██▋       | 304/1110 [01:10\u003c03:05,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.642, acc: 0.659:  29%|██▉       | 320/1110 [01:14\u003c03:04,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.649, acc: 0.652:  30%|███       | 336/1110 [01:17\u003c02:57,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.651, acc: 0.648:  32%|███▏      | 352/1110 [01:21\u003c02:54,  4.35it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.641:  33%|███▎      | 368/1110 [01:25\u003c02:52,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.641:  35%|███▍      | 384/1110 [01:28\u003c02:46,  4.36it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.660, acc: 0.640:  36%|███▌      | 400/1110 [01:32\u003c02:42,  4.37it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.669, acc: 0.642:  37%|███▋      | 416/1110 [01:36\u003c02:41,  4.30it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.665, acc: 0.641:  39%|███▉      | 432/1110 [01:39\u003c02:35,  4.36it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.667, acc: 0.638:  40%|████      | 448/1110 [01:43\u003c02:31,  4.38it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.662, acc: 0.640:  42%|████▏     | 464/1110 [01:46\u003c02:26,  4.41it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.642:  43%|████▎     | 480/1110 [01:50\u003c02:24,  4.37it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.659, acc: 0.639:  45%|████▍     | 496/1110 [01:54\u003c02:20,  4.36it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.657, acc: 0.637:  46%|████▌     | 512/1110 [01:57\u003c02:16,  4.37it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.640:  48%|████▊     | 528/1110 [02:01\u003c02:12,  4.39it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.652, acc: 0.645:  49%|████▉     | 544/1110 [02:05\u003c02:08,  4.39it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.648, acc: 0.650:  50%|█████     | 560/1110 [02:08\u003c02:05,  4.37it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.654, acc: 0.644:  52%|█████▏    | 576/1110 [02:12\u003c02:01,  4.40it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.642:  53%|█████▎    | 592/1110 [02:16\u003c01:57,  4.41it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.646:  55%|█████▍    | 608/1110 [02:19\u003c01:53,  4.42it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.649, acc: 0.651:  56%|█████▌    | 624/1110 [02:23\u003c01:50,  4.41it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.647, acc: 0.653:  58%|█████▊    | 640/1110 [02:27\u003c01:48,  4.34it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.647, acc: 0.652:  59%|█████▉    | 656/1110 [02:32\u003c01:56,  3.90it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.646, acc: 0.653:  61%|██████    | 672/1110 [02:36\u003c01:51,  3.95it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.647, acc: 0.653:  62%|██████▏   | 688/1110 [02:39\u003c01:43,  4.06it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.646, acc: 0.653:  63%|██████▎   | 704/1110 [02:43\u003c01:37,  4.16it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.647, acc: 0.653:  65%|██████▍   | 720/1110 [02:47\u003c01:34,  4.12it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.649:  66%|██████▋   | 736/1110 [02:51\u003c01:30,  4.15it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.650, acc: 0.648:  68%|██████▊   | 752/1110 [02:54\u003c01:24,  4.22it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.643:  69%|██████▉   | 768/1110 [02:58\u003c01:20,  4.27it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.644:  71%|███████   | 784/1110 [03:02\u003c01:15,  4.31it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.642:  72%|███████▏  | 800/1110 [03:05\u003c01:12,  4.29it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.643:  74%|███████▎  | 816/1110 [03:09\u003c01:08,  4.29it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.642:  75%|███████▍  | 832/1110 [03:13\u003c01:04,  4.30it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.640:  76%|███████▋  | 848/1110 [03:17\u003c01:01,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.639:  78%|███████▊  | 864/1110 [03:20\u003c00:57,  4.27it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.654, acc: 0.640:  79%|███████▉  | 880/1110 [03:24\u003c00:54,  4.25it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.656, acc: 0.640:  81%|████████  | 896/1110 [03:28\u003c00:49,  4.30it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.640:  82%|████████▏ | 912/1110 [03:32\u003c00:46,  4.29it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.641:  84%|████████▎ | 928/1110 [03:35\u003c00:42,  4.32it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.657, acc: 0.638:  85%|████████▌ | 944/1110 [03:39\u003c00:37,  4.43it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.657, acc: 0.635:  86%|████████▋ | 960/1110 [03:42\u003c00:33,  4.44it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.655, acc: 0.637:  88%|████████▊ | 976/1110 [03:46\u003c00:30,  4.34it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.633:  89%|████████▉ | 992/1110 [03:50\u003c00:27,  4.34it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.634:  91%|█████████ | 1008/1110 [03:53\u003c00:23,  4.32it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.636:  92%|█████████▏| 1024/1110 [03:57\u003c00:19,  4.33it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.657, acc: 0.637:  94%|█████████▎| 1040/1110 [04:01\u003c00:16,  4.28it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.658, acc: 0.635:  95%|█████████▌| 1056/1110 [04:05\u003c00:12,  4.30it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.659, acc: 0.634:  97%|█████████▋| 1072/1110 [04:08\u003c00:08,  4.34it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.659, acc: 0.633:  98%|█████████▊| 1088/1110 [04:12\u003c00:05,  4.27it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.660, acc: 0.632:  99%|█████████▉| 1104/1110 [04:16\u003c00:01,  4.29it/s]",
            "\n",
            "\rEpoch: 8 - loss: 0.660, acc: 0.632, val_loss: 0.867, val_acc: 0.625: 100%|██████████| 1110/1110 [04:45\u003c00:00,  1.63s/it]",
            "\rTraining:   1%|          | 9/1000 [1:08:09\u003c105:19:39, 382.62s/it]",
            "\n",
            "\rEpoch: 9:   0%|          | 0/1110 [00:00\u003c?, ?it/s]",
            "\n",
            "\rEpoch: 9 - loss: 1.203, acc: 0.688:   1%|▏         | 16/1110 [00:03\u003c04:16,  4.26it/s]",
            "\n",
            "\rEpoch: 9 - loss: 1.061, acc: 0.688:   3%|▎         | 32/1110 [00:07\u003c04:09,  4.31it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.900, acc: 0.708:   4%|▍         | 48/1110 [00:10\u003c04:03,  4.36it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.809, acc: 0.719:   6%|▌         | 64/1110 [00:14\u003c03:57,  4.41it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.772, acc: 0.675:   7%|▋         | 80/1110 [00:18\u003c04:07,  4.17it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.778, acc: 0.615:   9%|▊         | 96/1110 [00:22\u003c04:09,  4.06it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.763, acc: 0.607:  10%|█         | 112/1110 [00:27\u003c04:14,  3.92it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.742, acc: 0.617:  12%|█▏        | 128/1110 [00:31\u003c04:08,  3.95it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.717, acc: 0.632:  13%|█▎        | 144/1110 [00:35\u003c03:58,  4.05it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.708, acc: 0.637:  14%|█▍        | 160/1110 [00:38\u003c03:49,  4.14it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.698, acc: 0.642:  16%|█▌        | 176/1110 [00:42\u003c03:42,  4.19it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.702, acc: 0.625:  17%|█▋        | 192/1110 [00:46\u003c03:35,  4.25it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.691, acc: 0.620:  19%|█▊        | 208/1110 [00:49\u003c03:29,  4.31it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.682, acc: 0.634:  20%|██        | 224/1110 [00:53\u003c03:25,  4.31it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.676, acc: 0.646:  22%|██▏       | 240/1110 [00:57\u003c03:29,  4.15it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.680, acc: 0.648:  23%|██▎       | 256/1110 [01:01\u003c03:30,  4.06it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.677, acc: 0.651:  25%|██▍       | 272/1110 [01:05\u003c03:28,  4.02it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.671, acc: 0.653:  26%|██▌       | 288/1110 [01:09\u003c03:25,  4.00it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.668, acc: 0.651:  27%|██▋       | 304/1110 [01:14\u003c03:24,  3.95it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.664, acc: 0.647:  29%|██▉       | 320/1110 [01:18\u003c03:23,  3.89it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.664, acc: 0.652:  30%|███       | 336/1110 [01:22\u003c03:22,  3.83it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.660, acc: 0.651:  32%|███▏      | 352/1110 [01:27\u003c03:25,  3.68it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.659, acc: 0.649:  33%|███▎      | 368/1110 [01:31\u003c03:15,  3.79it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.656, acc: 0.656:  35%|███▍      | 384/1110 [01:35\u003c03:07,  3.87it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.648, acc: 0.660:  36%|███▌      | 400/1110 [01:38\u003c02:57,  3.99it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.650, acc: 0.654:  37%|███▋      | 416/1110 [01:42\u003c02:51,  4.05it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.655, acc: 0.655:  39%|███▉      | 432/1110 [01:46\u003c02:45,  4.09it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.655, acc: 0.654:  40%|████      | 448/1110 [01:50\u003c02:39,  4.15it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.648, acc: 0.659:  42%|████▏     | 464/1110 [01:54\u003c02:37,  4.11it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.644, acc: 0.660:  43%|████▎     | 480/1110 [01:58\u003c02:32,  4.14it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.649, acc: 0.659:  45%|████▍     | 496/1110 [02:02\u003c02:29,  4.09it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.646, acc: 0.660:  46%|████▌     | 512/1110 [02:05\u003c02:23,  4.17it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.644, acc: 0.659:  48%|████▊     | 528/1110 [02:09\u003c02:17,  4.23it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.645, acc: 0.656:  49%|████▉     | 544/1110 [02:13\u003c02:12,  4.28it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.642, acc: 0.657:  50%|█████     | 560/1110 [02:16\u003c02:08,  4.28it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.642, acc: 0.655:  52%|█████▏    | 576/1110 [02:20\u003c02:04,  4.29it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.644, acc: 0.654:  53%|█████▎    | 592/1110 [02:24\u003c01:59,  4.34it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.641, acc: 0.656:  55%|█████▍    | 608/1110 [02:27\u003c01:55,  4.33it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.640, acc: 0.659:  56%|█████▌    | 624/1110 [02:31\u003c01:53,  4.28it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.642, acc: 0.656:  58%|█████▊    | 640/1110 [02:35\u003c01:50,  4.27it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.641, acc: 0.655:  59%|█████▉    | 656/1110 [02:39\u003c01:45,  4.31it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.640, acc: 0.658:  61%|██████    | 672/1110 [02:42\u003c01:42,  4.27it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.641, acc: 0.657:  62%|██████▏   | 688/1110 [02:50\u003c02:07,  3.32it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.649, acc: 0.653:  63%|██████▎   | 704/1110 [02:59\u003c02:34,  2.63it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.653, acc: 0.649:  65%|██████▍   | 720/1110 [03:14\u003c03:38,  1.79it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.650, acc: 0.649:  66%|██████▋   | 736/1110 [03:26\u003c03:49,  1.63it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.649, acc: 0.652:  68%|██████▊   | 752/1110 [03:39\u003c03:59,  1.49it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.656, acc: 0.651:  69%|██████▉   | 768/1110 [03:50\u003c03:50,  1.49it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.663, acc: 0.651:  71%|███████   | 784/1110 [04:01\u003c03:38,  1.49it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.669, acc: 0.647:  72%|███████▏  | 800/1110 [04:12\u003c03:33,  1.45it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.670, acc: 0.646:  74%|███████▎  | 816/1110 [04:23\u003c03:20,  1.46it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.668, acc: 0.647:  75%|███████▍  | 832/1110 [04:34\u003c03:10,  1.46it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.670, acc: 0.646:  76%|███████▋  | 848/1110 [04:45\u003c02:58,  1.47it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.678, acc: 0.642:  78%|███████▊  | 864/1110 [04:55\u003c02:45,  1.49it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.682, acc: 0.642:  79%|███████▉  | 880/1110 [05:06\u003c02:34,  1.49it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.681, acc: 0.643:  81%|████████  | 896/1110 [05:16\u003c02:21,  1.52it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.680, acc: 0.643:  82%|████████▏ | 912/1110 [05:27\u003c02:10,  1.52it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.681, acc: 0.642:  84%|████████▎ | 928/1110 [05:33\u003c01:44,  1.74it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.679, acc: 0.644:  85%|████████▌ | 944/1110 [05:42\u003c01:36,  1.72it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.677, acc: 0.646:  86%|████████▋ | 960/1110 [05:48\u003c01:18,  1.92it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.680, acc: 0.647:  88%|████████▊ | 976/1110 [05:52\u003c00:58,  2.29it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.680, acc: 0.646:  89%|████████▉ | 992/1110 [05:56\u003c00:45,  2.62it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.682, acc: 0.642:  91%|█████████ | 1008/1110 [06:00\u003c00:34,  2.93it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.681, acc: 0.641:  92%|█████████▏| 1024/1110 [06:04\u003c00:26,  3.21it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.681, acc: 0.639:  94%|█████████▎| 1040/1110 [06:08\u003c00:20,  3.50it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.683, acc: 0.639:  95%|█████████▌| 1056/1110 [06:12\u003c00:14,  3.63it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.685, acc: 0.637:  97%|█████████▋| 1072/1110 [06:16\u003c00:10,  3.72it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.686, acc: 0.634:  98%|█████████▊| 1088/1110 [06:20\u003c00:05,  3.82it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.689, acc: 0.631:  99%|█████████▉| 1104/1110 [06:28\u003c00:02,  2.93it/s]",
            "\n",
            "\rEpoch: 9 - loss: 0.688, acc: 0.632, val_loss: 0.737, val_acc: 0.636: 100%|██████████| 1110/1110 [07:33\u003c00:00,  3.49s/it]",
            "\rTraining:   1%|          | 10/1000 [1:15:42\u003c111:04:00, 403.88s/it]",
            "\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "\u003ckeras.callbacks.History at 0x1ca1b1e9fd0\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 23
        }
      ],
      "source": "model.fit([\n    padded_PR_train,\n    padded_RT_train,\n    X_train[[\u0027VL-t0\u0027, \u0027CD4-t0\u0027]]\n    ],\n    y_train,\n    validation_data\u003d([\n                        padded_PR_test,\n                        padded_RT_test,\n                        X_test[[\u0027VL-t0\u0027, \u0027CD4-t0\u0027]]\n                        ],\n                        y_test),\n    epochs\u003depochs,\n    batch_size\u003dbatch_size,\n    verbose\u003d0,\n    callbacks\u003d[early_stopping, checkpoint, TQDMCallback()]\n)\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}